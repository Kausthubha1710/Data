import os
import re
from datetime import datetime

def load_config(config_file):
    config = {}
    if not os.path.exists(config_file):
        raise FileNotFoundError(f"Config file not found: {config_file}")
    
    with open(config_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line and '=' in line:
                key, value = line.split('=', 1)
                config[key.strip()] = value.strip()
    return config

def get_all_drives():
    drives = []
    for drive in range(ord('A'), ord('Z') + 1):
        drive_letter = chr(drive) + ":\\"
        if os.path.exists(drive_letter):
            drives.append(drive_letter)
    return drives

def read_input_paths(input_file):
    if not input_file or not os.path.exists(input_file):
        print("INPUT_FILE not found or not defined, using common user directories.")
        return get_all_drives()
    
    with open(input_file, 'r', encoding='utf-8') as f:
        paths = [line.strip() for line in f if line.strip() and os.path.exists(line.strip())]
        return paths if paths else get_all_drives()

def get_creation_time(path):
    try:
        return os.path.getctime(path)
    except Exception:
        return None

def compile_patterns(pattern_str):
    patterns = [p.strip() for p in pattern_str.split(',') if p.strip()]
    return [re.compile(p) for p in patterns]

def scan_and_generate(config):
    input_file = config.get('INPUT_FILE')
    output_dir = config.get('OUTPUT_DIR')
    expected_arrival_time = config.get('EXPECTED_ARRIVAL_TIME', '')
    pattern_str = config.get('FILENAME_PATTERNS')
    
    if not pattern_str:
        raise ValueError("FILENAME_PATTERNS must be defined in config.txt")
    
    compiled_patterns = compile_patterns(pattern_str)
    
    if not output_dir or not os.path.isdir(output_dir):
        print("OUTPUT_DIR not defined or invalid in config.txt. Using Downloads folder as fallback.")
        current_user = os.getlogin()
        output_dir = os.path.join("C:\\Users", current_user, "Downloads")
    
    output_txt = os.path.join(output_dir, "ppredefinedfiles.txt")
    os.makedirs(output_dir, exist_ok=True)
    
    directories = read_input_paths(input_file)
    
    # Load existing entries - use both filename and full path for better matching
    existing_data = {}
    existing_by_filename = {}  # Secondary index by filename only
    
    if os.path.exists(output_txt):
        with open(output_txt, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                parts = line.strip().split('\t')
                if len(parts) >= 4:
                    filename = parts[0]
                    full_path = parts[2] if parts[2] != 'File Not Found' else f"missing_{line_num}"
                    
                    # Primary index by full path
                    existing_data[full_path] = parts
                    
                    # Secondary index by filename for handling duplicates
                    if filename not in existing_by_filename:
                        existing_by_filename[filename] = []
                    existing_by_filename[filename].append((full_path, parts))
    
    # Find current matching files - ensure no duplicates
    current_files = {}
    processed_files = set()  # Track processed files to avoid duplicates
    
    for root_dir in directories:
        for dirpath, _, filenames in os.walk(root_dir):
            if "summaries" in dirpath.lower():
                continue  # Skip the "summaries" folder
            
            for filename in filenames:
                if any(p.match(filename) for p in compiled_patterns):
                    full_path = os.path.join(dirpath, filename)
                    
                    # Normalize path to avoid duplicates from path variations
                    normalized_path = os.path.normpath(full_path)
                    
                    # Skip if we've already processed this exact file
                    if normalized_path in processed_files:
                        continue
                    
                    processed_files.add(normalized_path)
                    
                    ctime = get_creation_time(full_path)
                    if ctime:
                        actual_dt = datetime.fromtimestamp(ctime)
                        actual_time = actual_dt.strftime("%Y-%m-%d %H:%M:%S")
                        
                        # Combine the current date with the fixed time from the config
                        current_date = datetime.now().strftime("%Y-%m-%d")
                        expected_time = f"{current_date} {expected_arrival_time}"
                        
                        current_files[normalized_path] = [filename, actual_time, normalized_path, expected_time]
    
    # Build final entries - handle duplicates intelligently
    final_entries = []
    processed_filenames = set()  # Track by filename to avoid duplicates
    
    # First, process all currently existing files
    for full_path, file_data in current_files.items():
        filename = file_data[0]
        if filename not in processed_filenames:
            final_entries.append('\t'.join(file_data))
            processed_filenames.add(filename)
    
    # Then, process files that existed before but are now missing
    for old_path, old_data in existing_data.items():
        filename = old_data[0]
        
        # Skip if this filename is already processed (file still exists)
        if filename in processed_filenames:
            continue
            
        # Check if this file actually exists anywhere in current scan
        file_still_exists = any(current_files[path][0] == filename for path in current_files)
        
        if not file_still_exists:
            # File was previously tracked but now missing
            missing_entry = [
                filename,
                'File Not Found',
                'File Not Found', 
                f"{datetime.now().strftime('%Y-%m-%d')} {expected_arrival_time}"
            ]
            
            final_entries.append('\t'.join(missing_entry))
            processed_filenames.add(filename)
    
    # Write the updated file
    with open(output_txt, 'w', encoding='utf-8') as f:
        for entry in sorted(final_entries):
            f.write(entry + '\n')
    
    print(f"Updated ppredefinedfiles.txt saved to: {output_txt}")
    print(f"Total unique files tracked: {len(final_entries)}")
    
    # Debug info
    current_count = len(current_files)
    missing_count = len(final_entries) - current_count
    print(f"Current files found: {current_count}")
    print(f"Missing files tracked: {missing_count}")

# Entry Point
if __name__ == "__main__":
    config = load_config('config.txt')
    scan_and_generate(config)
