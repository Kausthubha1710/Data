1. Modified xml_parser.py
python
import os
import json
import xml.etree.ElementTree as ET
from collections import OrderedDict
from datetime import date

class XmlParser:
    def __init__(self, xml_file, collection_config):
        self.collection_config = collection_config
        self.xml_file = xml_file

    def parse_xml_file(self):
        try:
            tree = ET.parse(self.xml_file)
            root = tree.getroot()
            namespace = root.tag.split('}')[0].strip('{')
            ns_prefix = f'{{{namespace}}}' if namespace else ''
            collection_element = root.find(f'.//{ns_prefix}{self.collection_config["collections"][0]}')
            if collection_element is None:
                print(f"Collection not found: {self.collection_config['collections'][0]} in {self.xml_file}")
                return None
            data = []
            elements = collection_element.findall(f'{ns_prefix}*')
            attribute_name = self.collection_config["attribute_names"][0]
            attribute_values = self.collection_config["attribute_values"]
            for element in elements:
                if attribute_name in element.attrib and element.attrib.get(attribute_name) in attribute_values:
                    row = OrderedDict()
                    row["id"] = len(data) + 1
                    for attr in element.attrib:
                        row[attr] = element.attrib.get(attr, "")
                    row["ImportDate"] = date.today().strftime("%Y-%m-%d")
                    row["Assignee"] = ""
                    row["Action"] = ""
                    row["Remediation"] = ""
                    row["Comments"] = ""
                    row["Status"] = ""
                    data.append(row)
            return data
        except Exception as e:
            print(f"Error parsing {self.xml_file}: {e}")
            return None

    def parse_xml_file_metadata(self, db_manager, main_table_name):
        """Parse XML and return metadata rows based on main table data"""
        try:
            # Read main data from PostgreSQL table
            main_data = db_manager.load_table_data(main_table_name)
            if not main_data:
                print(f"No main data found in table {main_table_name}")
                return None

            metadata_rows = []
            today = date.today().strftime("%Y-%m-%d")

            # For each record in main table, create a metadata entry
            for main_row in main_data:
                metadata_row = OrderedDict()
                metadata_row['main_id'] = main_row['id']  # ID from main table
                metadata_row['SafeName'] = main_row.get('SafeName', '')
                metadata_row['present_on'] = today
                
                metadata_rows.append(metadata_row)

            return metadata_rows

        except Exception as e:
            print(f"Error parsing metadata from {self.xml_file}: {e}")
            return None
2. Modified metadata_updator.py
python
from db_manager import DBManager
from datetime import date

class MetadataUpdater:
    def __init__(self, metadata_data, db_manager, table_name, collection_config):
        self.metadata_data = metadata_data
        self.db_manager = db_manager
        self.table_name = table_name
        self.collection_config = collection_config

    def update_metadata(self):
        if not self.metadata_data:
            print(f"No metadata to update for table {self.table_name}")
            return

        self.db_manager.ensure_table(self.table_name, self.metadata_data[0])

        # Get the maximum ID from the database to continue the sequence
        existing_metadata = self.db_manager.load_metadata(self.table_name)
        max_existing_id = max((row['id'] for row in existing_metadata), default=0)

        # Build SafeId mappings from (main_id + SafeName) â†’ SafeId
        existing_safe_mappings = self.build_safe_id_mappings(existing_metadata)
        next_safe_id = max(existing_safe_mappings.values(), default=0) + 1

        # Filter new metadata to only include today's date
        today = date.today().strftime("%Y-%m-%d")
        new_metadata_today = [row for row in self.metadata_data if row.get('present_on') == today]

        if not new_metadata_today:
            print(f"No new metadata for today ({today}) in table {self.table_name}")
            return

        # Assign unique IDs and calculate SafeIds based on (main_id + SafeName)
        for i, row in enumerate(new_metadata_today, start=1):
            row['id'] = max_existing_id + i

            # Create key from main_id + SafeName
            main_id = row.get('main_id')
            safe_name = row.get('SafeName', '').lower()
            safe_key = (main_id, safe_name)

            # Assign SafeId, reuse existing or create new
            if safe_key in existing_safe_mappings:
                row['SafeId'] = existing_safe_mappings[safe_key]
            else:
                row['SafeId'] = next_safe_id
                existing_safe_mappings[safe_key] = next_safe_id
                next_safe_id += 1

        # Remove duplicates based on (main_id + SafeName + present_on)
        seen_combinations_today = set()
        unique_metadata_today = []

        for row in new_metadata_today:
            combination_key = (row['main_id'], row.get('SafeName', '').lower(), row.get('present_on', ''))
            if combination_key not in seen_combinations_today:
                seen_combinations_today.add(combination_key)
                unique_metadata_today.append(row)

        # Check if we already have entries for today's combinations
        existing_today = [row for row in existing_metadata if row.get('present_on') == today]
        existing_combinations_today = set()

        for row in existing_today:
            existing_combinations_today.add((row['main_id'], row.get('SafeName', '').lower(), row.get('present_on', '')))

        # Only insert combinations that don't already exist for today
        final_metadata = []

        for row in unique_metadata_today:
            combination_key = (row['main_id'], row.get('SafeName', '').lower(), row.get('present_on', ''))
            if combination_key not in existing_combinations_today:
                final_metadata.append(row)

        if final_metadata:
            self.db_manager.insert_rows(self.table_name, final_metadata)
            print(f"Inserted {len(final_metadata)} new metadata rows for {today} into {self.table_name}")
        else:
            print(f"No new unique metadata to insert for {today} in table {self.table_name}")

    def build_safe_id_mappings(self, existing_metadata):
        """Build mappings from (main_id + SafeName) to SafeId"""
        mappings = {}
        
        for row in existing_metadata:
            main_id = row.get('main_id')
            safe_name = row.get('SafeName', '').lower()
            if main_id is not None and safe_name and 'SafeId' in row:
                key = (main_id, safe_name)
                mappings[key] = row['SafeId']
        
        return mappings
3. Modified table_processor.py
python
import os
import re
from datetime import date
from xml_parser import XmlParser
from db_manager import DBManager
from config_loader import load_config
from metadata_updater import MetadataUpdater

class TableProcessor:
    def __init__(self, xml_dir, output_dir, collection_configs, db_manager):
        self.xml_dir = xml_dir
        self.output_dir = output_dir
        self.collection_configs = collection_configs
        self.db_manager = db_manager
        self.known_prefixes = ["PAM Dashboard Management"]

    def process_main_table(self):
        recent_files = self.get_most_recent_files()
        for config_name, (file_path, timestamp) in recent_files.items():
            print(f"Processing MAIN table file {os.path.basename(file_path)} using config {config_name}")
            xml_parser = XmlParser(file_path, self.collection_configs[config_name])
            data = xml_parser.parse_xml_file()
            if data:
                table_name = f"pam_cyberark_{config_name.replace(' ', '_').lower()}"
                self.db_manager.ensure_table(table_name, data[0])
                self.db_manager.delete_existing_data(table_name)
                self.db_manager.insert_rows(table_name, data)
                print(f"Inserted {len(data)} rows into {table_name}")
                print(f"Data loaded into pgAdmin table: {table_name}")
            else:
                print(f"[INFO] No rows extracted from {file_path}")

    def process_metadata_table(self):
        recent_files = self.get_most_recent_files()
        for config_name, (file_path, timestamp) in recent_files.items():
            print(f"Processing file {os.path.basename(file_path)} using config {config_name}")
            table_name = f"pam_cyberark_{config_name.replace(' ', '_').lower()}"
            metadata_table_name = f"pam_cyberark_metadata_{config_name.replace(' ', '_').lower()}"
            collection_config = self.collection_configs[config_name]
            
            # Process main table first to ensure it exists
            xml_parser = XmlParser(file_path, collection_config)
            main_data = xml_parser.parse_xml_file()
            if main_data:
                self.db_manager.ensure_table(table_name, main_data[0])
                self.db_manager.delete_existing_data(table_name)
                self.db_manager.insert_rows(table_name, main_data)
                print(f"Main table {table_name} updated with {len(main_data)} rows")
            
            # Now process metadata from the main table
            metadata_data = xml_parser.parse_xml_file_metadata(self.db_manager, table_name)
            if metadata_data:
                metadata_updater = MetadataUpdater(metadata_data, self.db_manager, metadata_table_name, collection_config)
                metadata_updater.update_metadata()
                print(f"Metadata loaded into pgAdmin table: {metadata_table_name}")
            else:
                print(f"[INFO] No metadata extracted from {file_path}")

    def get_most_recent_files(self):
        recent_files = {}
        for root, dirs, files in os.walk(self.xml_dir):
            for file in files:
                if not file.lower().endswith(".xml"):
                    continue
                basename = os.path.splitext(file)[0]
                base_name = self.strip_known_prefixes(basename)
                config_name = self.config_name_from_filename(base_name)
                if config_name is None:
                    print(f"[WARN] No config matched for file '{file}'. Skipping.")
                    continue
                full_path = os.path.join(root, file)
                timestamp = os.path.getctime(full_path)
                if config_name not in recent_files or timestamp > recent_files[config_name][1]:
                    recent_files[config_name] = (full_path, timestamp)
        return recent_files

    def strip_known_prefixes(self, name):
        for p in self.known_prefixes:
            if name.startswith(p):
                return name[len(p):].strip()
        return name

    def config_name_from_filename(self, filename_without_ext):
        cleaned_filename_without_ext = filename_without_ext.replace("_", "").replace("-", "").strip()
        tokens = [t for t in re.split(r"\s+", cleaned_filename_without_ext) if t]
        for key in self.collection_configs:
            key_norm = key.lower().replace("_", "").strip()
            key_parts = [p for p in re.split(r"\s+|[-]", key_norm) if p]
            if all(part in (t.lower() for t in tokens) for part in key_parts):
                return key
        return None
4. Modified db_manager.py 
import json
import os
from configparser import ConfigParser
from datetime import date
from psycopg2.extras import execute_values, Json
import psycopg2
from psycopg2 import sql

class DBManager:
    def __init__(self, credentials_path="db_credentials.txt"):
        self.creds = self.load_credentials(credentials_path)
        self.conn = self.connect()
        self.schema = self.creds.get("schema", "public")

    def load_credentials(self, path):
        creds = {}
        with open(path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and '=' in line:
                    key, value = line.split('=')
                    creds[key.strip()] = value.strip()
        return creds

    def connect(self):
        return psycopg2.connect(
            host=self.creds["host"],
            port=self.creds["port"],
            dbname=self.creds["database"],
            user=self.creds["user"],
            password=self.creds["password"],
        )

    def ensure_table(self, table_name, sample_row):
        qualified_name = sql.Identifier(self.schema, table_name)
        column_defs = []
        for col, val in sample_row.items():
            pg_type = self.postgres_type(val)
            column_defs.append(sql.SQL("{} {}").format(sql.Identifier(col), sql.SQL(pg_type)))
        column_defs.append(sql.SQL('PRIMARY KEY ("id")'))
        column_defs_sql = sql.SQL(",").join(column_defs)
        create_table_sql = sql.SQL("CREATE TABLE IF NOT EXISTS {} ({})").format(qualified_name, column_defs_sql)
        with self.conn.cursor() as cur:
            cur.execute(create_table_sql)
            self.conn.commit()

    def postgres_type(self, value):
        if isinstance(value, bool):
            return "BOOLEAN"
        if isinstance(value, int):
            return "INTEGER"
        if isinstance(value, float):
            return "DOUBLE PRECISION"
        return "TEXT"

    def insert_rows(self, table_name, rows):
        if not rows:
            return
        existing_columns = self.get_existing_columns(table_name)
        all_columns = set()
        for r in rows:
            all_columns.update(r.keys())
        column_order = sorted(list(set(existing_columns) & set(all_columns)))
        values = [[row.get(col, None) for col in column_order] for row in rows]
        qualified_name = sql.Identifier(self.schema, table_name)
        cols = sql.SQL(",").join(map(sql.Identifier, column_order))
        update_assignments = sql.SQL(", ").join(sql.SQL('{}=EXCLUDED.{}').format(sql.Identifier(col), sql.Identifier(col)) for col in column_order if col != "id")
        insert_sql = sql.SQL("INSERT INTO {} ({}) VALUES %s ON CONFLICT (id) DO UPDATE SET {}").format(qualified_name, cols, update_assignments)
        with self.conn.cursor() as cur:
            execute_values(cur, insert_sql, values, page_size=100)
            self.conn.commit()

    def get_existing_columns(self, table_name):
        sql_query = """
            SELECT column_name FROM information_schema.columns 
            WHERE table_schema = %s AND table_name = %s;
        """
        with self.conn.cursor() as cur:
            cur.execute(sql_query, (self.schema, table_name))
            return [row[0] for row in cur.fetchall()]

    def delete_existing_data(self, table_name):
        sql_query = sql.SQL("DELETE FROM {}.{}").format(sql.Identifier(self.schema), sql.Identifier(table_name))
        with self.conn.cursor() as cur:
            cur.execute(sql_query)
            self.conn.commit()

    def write_metadata(self, table_name, rows):
        self.ensure_table(table_name, rows[0])
        sql_query = sql.SQL("INSERT INTO {}.{} ({}) VALUES %s").format(
            sql.Identifier(self.schema),
            sql.Identifier(table_name),
            sql.SQL(",").join(map(sql.Identifier, rows[0].keys()))
        )
        values = [[row[col] for col in rows[0].keys()] for row in rows]
        with self.conn.cursor() as cur:
            execute_values(cur, sql_query, values, page_size=100)
            self.conn.commit()

    def load_metadata(self, table_name):
        sql_query = sql.SQL("SELECT * FROM {}.{}").format(sql.Identifier(self.schema), sql.Identifier(table_name))
        with self.conn.cursor() as cur:
            cur.execute(sql_query)
            rows = cur.fetchall()
            columns = [desc[0] for desc in cur.description]
            return [dict(zip(columns, row)) for row in rows]

    def update_metadata(self, table_name, rows):
        self.ensure_table(table_name, rows[0])
        sql_query = sql.SQL("INSERT INTO {}.{} ({}) VALUES %s ON CONFLICT (id) DO UPDATE SET {}").format(
            sql.Identifier(self.schema),
            sql.Identifier(table_name),
            sql.SQL(",").join(map(sql.Identifier, rows[0].keys())),
            sql.SQL(", ").join(sql.SQL('{}=EXCLUDED.{}').format(sql.Identifier(col), sql.Identifier(col)) for col in rows[0].keys() if col != "id")
        )
        values = [[row[col] for col in rows[0].keys()] for row in rows]
        with self.conn.cursor() as cur:
            execute_values(cur, sql_query, values, page_size=100)
            self.conn.commit()

    def load_table_data(self, table_name):
        """Load all data from a table"""
        try:
            qualified_name = sql.Identifier(self.schema, table_name)
            sql_query = sql.SQL("SELECT * FROM {}").format(qualified_name)
            with self.conn.cursor() as cur:
                cur.execute(sql_query)
                rows = cur.fetchall()
                columns = [desc[0] for desc in cur.description]
                return [dict(zip(columns, row)) for row in rows]
        except Exception as e:
            print(f"Error loading data from {table_name}: {e}")
            return []

    def close(self):
        self.conn.close()
Key Changes:
Removed JSON dependency: Now directly reads from PostgreSQL tables

Simplified metadata parsing: parse_xml_file_metadata now reads from main table instead of complex XML parsing

Correct SafeId logic: Based on (main_id, SafeName) combination

Proper main table processing: Ensures main table is processed before metadata

Direct database operations: No intermediate JSON files
