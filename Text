def parse_xml_file_metadata(self, output_dir, metadata_table_name, table_name, existing_metadata, main_data):
    try:
        # Filter out metadata with the current date
        today = date.today().strftime("%Y-%m-%d")
        existing_metadata = [metadata for metadata in existing_metadata if metadata.get('present_on') != today]

        # Find the last ID in the existing metadata
        last_id = max((metadata['id'] for metadata in existing_metadata), default=0)

        # Initialize safe ID map and counter
        safe_id_map = {(row['id'], row.get('SafeName', '').lower()): row['id'] for row in main_data}
        next_safe_id = max(safe_id_map.values(), default=0) + 1

        tree = ET.parse(self.xml_file)
        root = tree.getroot()
        namespace = root.tag.split('}')[0].strip('{')
        ns_prefix = f'{{{namespace}}}' if namespace else ''
        collection_element = root.find(f'.//{ns_prefix}{self.collection_config["collections"][0]}')
        if collection_element is None:
            print(f"Collection not found: {self.collection_config['collections'][0]} in {self.xml_file}")
            return None

        metadata_rows = []
        elements = collection_element.findall(f'{ns_prefix}*')
        attribute_name = self.collection_config["attribute_names"][0]
        attribute_values = self.collection_config["attribute_values"]
        for element in elements:
            if attribute_name in element.attrib and element.attrib.get(attribute_name) in attribute_values:
                metadata_row = OrderedDict()
                safe_name = next((element.attrib.get(k, '') for k in element.attrib if k.lower() == 'safename'), '')
                metadata_row['id'] = last_id + 1
                key = (metadata_row['id'], safe_name.lower())
                if key not in safe_id_map:
                    safe_id_map[key] = next_safe_id
                    next_safe_id += 1
                metadata_row['SafeId'] = safe_id_map[key]
                metadata_row['SafeName'] = safe_name
                metadata_row['present_on'] = today
                metadata_rows.append(metadata_row)
                last_id += 1
        return metadata_rows
    except Exception as e:
        print(f"Error parsing metadata from {self.xml_file}: {e}")
        return None



















import ast

def load_config(config_file_path):
    config = {}
    collection_configs = {}
    with open(config_file_path, 'r') as f:
        for line in f:
            line = line.strip()
            if line and '=' in line:
                key, value = line.split('=', 1)  # Split only on first '=' to handle complex values
                key = key.strip()
                value = value.strip()
                if key in ['xml_dir', 'output_dir']:
                    config[key] = value
                else:
                    collection_configs[key] = {}
                    parts = [part.strip() for part in value.split(',')]
                    
                    # Parse collections
                    collection_configs[key]['collections'] = [parts[0].strip().replace('[', '').replace(']', '')]
                    
                    # Parse attribute_names
                    collection_configs[key]['attribute_names'] = [parts[1].strip().replace('[', '').replace(']', '')]
                    
                    # Parse attribute_values and metadata_columns
                    attribute_values = []
                    metadata_columns = None
                    
                    # Find where metadata column specification starts (contains '[id,')
                    metadata_start_index = None
                    for i, part in enumerate(parts):
                        if '[id,' in part:
                            metadata_start_index = i
                            break
                    
                    if metadata_start_index is not None:
                        # Everything before metadata_start_index is attribute values (except first 2 parts)
                        attribute_parts = parts[2:metadata_start_index]
                        metadata_part = ' '.join(parts[metadata_start_index:])  # Join all metadata parts
                        metadata_columns = parse_metadata_columns(metadata_part)
                    else:
                        # No metadata specification, all remaining parts are attribute values
                        attribute_parts = parts[2:]
                        metadata_columns = ['SafeName']  # Default
                    
                    # Parse attribute values - handle comma-separated values within parts
                    for part in attribute_parts:
                        # Remove any brackets first
                        clean_part = part.strip().replace('[', '').replace(']', '')
                        # Split by comma and add each value
                        values = [v.strip() for v in clean_part.split(',') if v.strip()]
                        attribute_values.extend(values)
                    
                    collection_configs[key]['attribute_values'] = attribute_values
                    collection_configs[key]['metadata_columns'] = metadata_columns or ['SafeName']  # Default
                    
    return config, collection_configs

def parse_metadata_columns(metadata_part):
    """
    Parse metadata column specification from config.
    Examples:
    - [id, ("SERVER_ID", "PAM_LOGINNAME")] -> ['SERVER_ID', 'PAM_LOGINNAME']
    - [id, (SafeName)] -> ['SafeName']
    """
    try:
        # Remove outer brackets and split by comma
        clean_part = metadata_part.strip().replace('[', '').replace(']', '')
        
        # Find the tuple part
        if '(' in clean_part and ')' in clean_part:
            # Extract content between parentheses
            start = clean_part.find('(') + 1
            end = clean_part.find(')')
            tuple_content = clean_part[start:end]
            
            # Split by comma and clean up
            columns = [col.strip().strip('"').strip("'") for col in tuple_content.split(',')]
            return [col for col in columns if col]  # Remove empty strings
        else:
            # No tuple format, look for column names after 'id,'
            parts = [p.strip() for p in clean_part.split(',')]
            if len(parts) > 1:
                return [parts[1]]  # Return the part after 'id,'
    except Exception as e:
        print(f"Warning: Could not parse metadata columns from '{metadata_part}': {e}")
        return ['SafeName']  # Default fallback
    
    return ['SafeName']  # Default fallback


















# config_loader.py
import re
import ast

def load_config(config_file_path):
    config = {}
    collection_configs = {}
    
    with open(config_file_path, 'r') as f:
        for line in f:
            line = line.strip()
            if line and '=' in line:
                key, value = line.split('=', 1)  # Split only on first '='
                key = key.strip()
                value = value.strip()
                
                if key in ['xml_dir', 'output_dir']:
                    config[key] = value
                else:
                    collection_configs[key] = parse_collection_config(value)
    
    return config, collection_configs

def parse_collection_config(config_value):
    """
    Parse configuration values like:
    [Details1_Collection], [metier], [CIB ITO IT Client Engagement & Protection IT], [id, ("SERVER_ID", "PAM_LOGINNAME")]
    """
    config_dict = {}
    
    # Split by comma and clean up brackets
    parts = [part.strip() for part in config_value.split(',')]
    
    # Extract collections (first part)
    if parts[0]:
        collections_str = parts[0].replace('[', '').replace(']', '').strip()
        config_dict['collections'] = [collections_str]
    
    # Extract attribute names (second part)
    if len(parts) > 1 and parts[1]:
        attr_str = parts[1].replace('[', '').replace(']', '').strip()
        config_dict['attribute_names'] = [attr_str]
    
    # Extract attribute values (third part)
    if len(parts) > 2 and parts[2]:
        attr_values_str = parts[2].replace('[', '').replace(']', '').strip()
        config_dict['attribute_values'] = [attr_values_str]
    
    # Extract metadata columns configuration (fourth part and beyond)
    if len(parts) > 3:
        metadata_config_str = ', '.join(parts[3:]).replace('[', '').replace(']', '').strip()
        config_dict['metadata_columns'] = parse_metadata_columns(metadata_config_str)
    else:
        # Default to legacy SafeName behavior
        config_dict['metadata_columns'] = {
            'base_columns': ['id'],
            'custom_columns': ['SafeName'],
            'safe_id_source': ['SafeName']
        }
    
    return config_dict

def parse_metadata_columns(metadata_str):
    """
    Parse metadata column configuration like:
    id, ("SERVER_ID", "PAM_LOGINNAME")
    """
    metadata_config = {
        'base_columns': [],
        'custom_columns': [],
        'safe_id_source': []
    }
    
    try:
        # Use regex to find tuples and individual items
        tuple_pattern = r'\([^)]+\)'
        tuples = re.findall(tuple_pattern, metadata_str)
        
        # Remove tuples from string to get individual items
        remaining_str = re.sub(tuple_pattern, '', metadata_str)
        individual_items = [item.strip() for item in remaining_str.split(',') if item.strip()]
        
        # Process individual items (base columns)
        for item in individual_items:
            if item and item not in ['', '(', ')']:
                metadata_config['base_columns'].append(item)
        
        # Process tuples (custom columns for SafeID calculation)
        for tuple_str in tuples:
            # Parse the tuple string
            try:
                # Clean up the tuple string and evaluate it
                clean_tuple = tuple_str.replace('(', '').replace(')', '').replace('"', '').replace("'", '')
                columns = [col.strip() for col in clean_tuple.split(',')]
                metadata_config['custom_columns'].extend(columns)
                metadata_config['safe_id_source'].extend(columns)
            except:
                print(f"Warning: Could not parse tuple {tuple_str}")
        
        # If no custom columns specified, default to SafeName
        if not metadata_config['custom_columns']:
            metadata_config['custom_columns'] = ['SafeName']
            metadata_config['safe_id_source'] = ['SafeName']
            
    except Exception as e:
        print(f"Error parsing metadata columns: {e}")
        # Default fallback
        metadata_config = {
            'base_columns': ['id'],
            'custom_columns': ['SafeName'],
            'safe_id_source': ['SafeName']
        }
    
    return metadata_config


# xml_parser.py
import os
import xml.etree.ElementTree as ET
import json
from collections import OrderedDict
from datetime import date

class XmlParser:
    def __init__(self, xml_file, collection_config):
        self.xml_file = xml_file
        self.collection_config = collection_config

    def parse_xml_file(self):
        try:
            tree = ET.parse(self.xml_file)
            root = tree.getroot()
            namespace = root.tag.split('}')[0].strip('{')
            ns_prefix = f'{{{namespace}}}' if namespace else ''
            collection_element = root.find(f'.//{ns_prefix}{self.collection_config["collections"][0]}')
            
            if collection_element is None:
                print(f"Collection not found: {self.collection_config['collections'][0]} in {self.xml_file}")
                return None
            
            data = []
            elements = collection_element.findall(f'{ns_prefix}*')
            attribute_name = self.collection_config["attribute_names"][0]
            attribute_values = self.collection_config["attribute_values"]
            
            for element in elements:
                if attribute_name in element.attrib and element.attrib.get(attribute_name) in attribute_values:
                    row = OrderedDict()
                    row['id'] = len(data) + 1
                    
                    for attr in element.attrib:
                        row[attr] = element.attrib.get(attr, '')
                    
                    row['ImportDate'] = date.today().strftime("%Y-%m-%d")
                    row['Assignee'] = ''
                    row['Action'] = ''
                    row['Remediation'] = ''
                    row['Comments'] = ''
                    row['Status'] = ""
                    data.append(row)
            
            return data
        except Exception as e:
            print(f"Error parsing {self.xml_file}: {e}")
            return None

    def parse_xml_file_metadata(self, output_dir, metadata_table_name, table_name):
        try:
            tree = ET.parse(self.xml_file)
            root = tree.getroot()
            namespace = root.tag.split('}')[0].strip('{')
            ns_prefix = f'{{{namespace}}}' if namespace else ''
            collection_element = root.find(f'.//{ns_prefix}{self.collection_config["collections"][0]}')
            
            if collection_element is None:
                print(f"Collection not found: {self.collection_config['collections'][0]} in {self.xml_file}")
                return None
            
            metadata_rows = []
            today = date.today().strftime("%Y-%m-%d")
            
            # Get metadata column configuration
            metadata_config = self.collection_config.get('metadata_columns', {
                'base_columns': ['id'],
                'custom_columns': ['SafeName'],
                'safe_id_source': ['SafeName']
            })
            
            # Get elements from XML
            elements = collection_element.findall(f'{ns_prefix}*')
            attribute_name = self.collection_config["attribute_names"][0]
            attribute_values = self.collection_config["attribute_values"]
            
            # Track unique combinations to avoid duplicates
            seen_combinations = set()
            temp_id = 1
            
            for element in elements:
                if attribute_name in element.attrib and element.attrib.get(attribute_name) in attribute_values:
                    
                    # Extract values for custom columns
                    column_values = []
                    for col_name in metadata_config['safe_id_source']:
                        # Try different case variations to find the attribute
                        value = self._find_attribute_value(element.attrib, col_name)
                        column_values.append(value)
                    
                    # Create combination key for duplicate detection
                    combination_key = tuple(column_values + [today])
                    
                    # Skip if we've already seen this combination today
                    if combination_key in seen_combinations:
                        continue
                    seen_combinations.add(combination_key)
                    
                    # Create metadata row
                    metadata_row = OrderedDict()
                    
                    # Add base columns
                    for base_col in metadata_config['base_columns']:
                        if base_col == 'id':
                            metadata_row['id'] = temp_id
                        else:
                            metadata_row[base_col] = self._find_attribute_value(element.attrib, base_col)
                    
                    # Add custom columns
                    for custom_col in metadata_config['custom_columns']:
                        metadata_row[custom_col] = self._find_attribute_value(element.attrib, custom_col)
                    
                    # Calculate SafeID based on configured source columns
                    metadata_row['SafeID'] = self._calculate_safe_id(temp_id, column_values, metadata_config)
                    
                    # Always add present_on
                    metadata_row['present_on'] = today
                    
                    metadata_rows.append(metadata_row)
                    temp_id += 1
            
            return metadata_rows
            
        except Exception as e:
            print(f"Error parsing metadata from {self.xml_file}: {e}")
            return None
    
    def _find_attribute_value(self, attributes, column_name):
        """
        Find attribute value with case-insensitive matching
        """
        # Direct match first
        if column_name in attributes:
            return attributes[column_name]
        
        # Case-insensitive search
        for attr_name, attr_value in attributes.items():
            if attr_name.lower() == column_name.lower():
                return attr_value
        
        return ''  # Return empty string if not found
    
    def _calculate_safe_id(self, base_id, column_values, metadata_config):
        """
        Calculate SafeID based on configuration
        """
        if len(metadata_config['safe_id_source']) == 1:
            # Single column - use simple approach
            return f"{base_id}_{column_values[0]}" if column_values[0] else str(base_id)
        else:
            # Multiple columns - combine them
            non_empty_values = [str(val) for val in column_values if val]
            if non_empty_values:
                combined = "_".join(non_empty_values)
                return f"{base_id}_{combined}"
            else:
                return str(base_id)


# metadata_updater.py
from db_manager import DBManager
from datetime import date

class MetadataUpdater:
    def __init__(self, metadata_data, db_manager, table_name):
        self.metadata_data = metadata_data
        self.db_manager = db_manager
        self.table_name = table_name

    def update_metadata(self):
        if not self.metadata_data:
            print(f"No metadata to update for table {self.table_name}")
            return
            
        self.db_manager.ensure_table(self.table_name, self.metadata_data[0])
        
        # Get the maximum ID from the database to continue the sequence
        existing_metadata = self.db_manager.load_metadata(self.table_name)
        max_existing_id = max((row['id'] for row in existing_metadata), default=0)
        
        # Filter new metadata to only include today's date
        today = date.today().strftime("%Y-%m-%d")
        new_metadata_today = [row for row in self.metadata_data if row.get('present_on') == today]
        
        if not new_metadata_today:
            print(f"No new metadata for today ({today}) in table {self.table_name}")
            return
        
        # Assign unique IDs starting from max_existing_id + 1
        for i, row in enumerate(new_metadata_today, start=1):
            row['id'] = max_existing_id + i
            # Recalculate SafeID with new ID if needed
            if 'SafeID' in row:
                safe_id_parts = row['SafeID'].split('_')
                if safe_id_parts and safe_id_parts[0].isdigit():
                    # Update the ID part of SafeID
                    safe_id_parts[0] = str(row['id'])
                    row['SafeID'] = '_'.join(safe_id_parts)
        
        # Remove duplicates based on SafeID and present_on (more flexible than SafeName)
        seen_safe_ids_today = set()
        unique_metadata_today = []
        
        for row in new_metadata_today:
            safe_key = (row.get('SafeID', ''), row.get('present_on', ''))
            if safe_key not in seen_safe_ids_today:
                seen_safe_ids_today.add(safe_key)
                unique_metadata_today.append(row)
        
        # Check if we already have entries for today's SafeIDs
        existing_today = [row for row in existing_metadata if row.get('present_on') == today]
        existing_safe_ids_today = set((row.get('SafeID', ''), row.get('present_on', '')) for row in existing_today)
        
        # Only insert SafeIDs that don't already exist for today
        final_metadata = []
        for row in unique_metadata_today:
            safe_key = (row.get('SafeID', ''), row.get('present_on', ''))
            if safe_key not in existing_safe_ids_today:
                final_metadata.append(row)
        
        if final_metadata:
            self.db_manager.insert_rows(self.table_name, final_metadata)
            print(f"Inserted {len(final_metadata)} new metadata rows for {today} into {self.table_name}")
        else:
            print(f"No new unique metadata to insert for {today} in table {self.table_name}")


# Example updated config.txt format:
# xml_dir=/path/to/xml/files
# output_dir=/path/to/output
# INFORA02=[Details1_Collection], [metier], [CIB ITO IT Client Engagement & Protection IT], [id, ("SERVER_ID", "PAM_LOGINNAME")]
# SA09=[Details2_Collection], [Metier2], [CIB ITO IT Client Engagement & Protection IT, CIB ITO IT CIB ITO CCCO], [id, ("SafeName")]


# db_manager.py (minimal changes needed - mostly compatible)
import json
import os
from configparser import ConfigParser
from datetime import date
import psycopg2
from psycopg2 import sql
from psycopg2.extras import execute_values, Json

class DBManager:
    def __init__(self, credentials_path="db_credentials.txt"):
        self.creds = self.load_credentials(credentials_path)
        self.conn = self.connect()
        self.schema = self.creds.get("schema", "public")

    def load_credentials(self, path):
        creds = {}
        with open(path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and '=' in line:
                    key, value = line.split('=')
                    creds[key.strip()] = value.strip()
        return creds

    def connect(self):
        return psycopg2.connect(
            host=self.creds["host"],
            port=self.creds["port"],
            dbname=self.creds["database"],
            user=self.creds["user"],
            password=self.creds["password"],
        )

    def ensure_table(self, table_name, sample_row):
        qualified_name = sql.Identifier(self.schema, table_name)
        column_defs = []
        for col, val in sample_row.items():
            pg_type = self.postgres_type(val)
            column_defs.append(sql.SQL("{} {}").format(sql.Identifier(col), sql.SQL(pg_type)))
        column_defs.append(sql.SQL('PRIMARY KEY ("id")'))
        column_defs_sql = sql.SQL(", ").join(column_defs)
        create_table_sql = sql.SQL("CREATE TABLE IF NOT EXISTS {} ({})").format(qualified_name, column_defs_sql)
        with self.conn.cursor() as cur:
            cur.execute(create_table_sql)
            self.conn.commit()

    def postgres_type(self, value):
        if isinstance(value, bool):
            return "BOOLEAN"
        if isinstance(value, int):
            return "INTEGER"
        if isinstance(value, float):
            return "DOUBLE PRECISION"
        return "TEXT"

    def insert_rows(self, table_name, rows):
        if not rows:
            return
        existing_columns = self.get_existing_columns(table_name)
        all_columns = set()
        for r in rows:
            all_columns.update(r.keys())
        column_order = sorted(list(set(existing_columns) & set(all_columns)))
        values = [[row.get(col, None) for col in column_order] for row in rows]
        qualified_name = sql.Identifier(self.schema, table_name)
        cols = sql.SQL(",").join(map(sql.Identifier, column_order))
        update_assignments = sql.SQL(", ").join(sql.SQL('{}=EXCLUDED.{}').format(sql.Identifier(col), sql.Identifier(col)) for col in column_order if col != "id")
        insert_sql = sql.SQL("INSERT INTO {} ({}) VALUES %s ON CONFLICT (id) DO UPDATE SET {}").format(qualified_name, cols, update_assignments)
        with self.conn.cursor() as cur:
            execute_values(cur, insert_sql, values, page_size=100)
            self.conn.commit()

    def get_existing_columns(self, table_name):
        sql_query = """
            SELECT column_name
            FROM information_schema.columns
            WHERE table_schema = %s
            AND table_name = %s;
        """
        with self.conn.cursor() as cur:
            cur.execute(sql_query, (self.schema, table_name))
            return [row[0] for row in cur.fetchall()]

    def delete_existing_data(self, table_name):
        sql_query = sql.SQL("DELETE FROM {}.{}").format(sql.Identifier(self.schema), sql.Identifier(table_name))
        with self.conn.cursor() as cur:
            cur.execute(sql_query)
            self.conn.commit()

    def write_metadata(self, table_name, rows):
        self.ensure_table(table_name, rows[0])
        sql_query = sql.SQL("INSERT INTO {}.{} ({}) VALUES %s").format(sql.Identifier(self.schema), sql.Identifier(table_name), sql.SQL(",").join(map(sql.Identifier, rows[0].keys())))
        values = [[row[col] for col in rows[0].keys()] for row in rows]
        with self.conn.cursor() as cur:
            execute_values(cur, sql_query, values, page_size=100)
            self.conn.commit()

    def load_metadata(self, table_name):
        sql_query = sql.SQL("SELECT * FROM {}.{}").format(sql.Identifier(self.schema), sql.Identifier(table_name))
        with self.conn.cursor() as cur:
            cur.execute(sql_query)
            rows = cur.fetchall()
            columns = [desc[0] for desc in cur.description]
            return [dict(zip(columns, row)) for row in rows]

    def update_metadata(self, table_name, rows):
        self.ensure_table(table_name, rows[0])
        sql_query = sql.SQL("INSERT INTO {}.{} ({}) VALUES %s ON CONFLICT (id) DO UPDATE SET {}").format(
            sql.Identifier(self.schema),
            sql.Identifier(table_name),
            sql.SQL(",").join(map(sql.Identifier, rows[0].keys())),
            sql.SQL(", ").join(sql.SQL('{}=EXCLUDED.{}').format(sql.Identifier(col), sql.Identifier(col)) for col in rows[0].keys() if col != "id")
        )
        values = [[row[col] for col in rows[0].keys()] for row in rows]
        with self.conn.cursor() as cur:
            execute_values(cur, sql_query, values, page_size=100)
            self.conn.commit()

    def close(self):
        self.conn.close()


































































































config_loader.py

import ast

def load_config(config_file_path):
    config = {}
    collection_configs = {}
    with open(config_file_path, 'r') as f:
        for line in f:
            line = line.strip()
            if line and '=' in line:
                key, value = line.split('=', 1)  # Split only on first '=' to handle complex values
                key = key.strip()
                value = value.strip()
                if key in ['xml_dir', 'output_dir']:
                    config[key] = value
                else:
                    collection_configs[key] = {}
                    parts = [part.strip() for part in value.split(',')]
                    
                    # Parse collections
                    collection_configs[key]['collections'] = [parts[0].strip().replace('[', '').replace(']', '')]
                    
                    # Parse attribute_names
                    collection_configs[key]['attribute_names'] = [parts[1].strip().replace('[', '').replace(']', '')]
                    
                    # Parse attribute_values (can be multiple)
                    attribute_values = []
                    metadata_columns = None
                    
                    # Look for metadata column specification in parts[3] if it exists
                    for i, part in enumerate(parts[2:], start=2):
                        # Check if this part contains metadata column specification
                        if i == 3 and '[id,' in part:
                            # This is the metadata column specification
                            # Extract the metadata columns part
                            metadata_part = part
                            # Parse the metadata column specification
                            metadata_columns = parse_metadata_columns(metadata_part)
                        else:
                            # This is an attribute value
                            if not ('[id,' in part):  # Skip metadata specification parts
                                attribute_values.extend([p.strip() for p in part.split(',')])
                    
                    # If no attribute values found in parts[2], parse it normally
                    if not attribute_values and len(parts) > 2:
                        if not ('[id,' in parts[2]):
                            attribute_values.extend([p.strip() for p in parts[2].split(',')])
                    
                    collection_configs[key]['attribute_values'] = attribute_values
                    collection_configs[key]['metadata_columns'] = metadata_columns or ['SafeName']  # Default
                    
    return config, collection_configs

def parse_metadata_columns(metadata_part):
    """
    Parse metadata column specification from config.
    Examples:
    - [id, ("SERVER_ID", "PAM_LOGINNAME")] -> ['SERVER_ID', 'PAM_LOGINNAME']
    - [id, (SafeName)] -> ['SafeName']
    """
    try:
        # Remove outer brackets and split by comma
        clean_part = metadata_part.strip().replace('[', '').replace(']', '')
        
        # Find the tuple part
        if '(' in clean_part and ')' in clean_part:
            # Extract content between parentheses
            start = clean_part.find('(') + 1
            end = clean_part.find(')')
            tuple_content = clean_part[start:end]
            
            # Split by comma and clean up
            columns = [col.strip().strip('"').strip("'") for col in tuple_content.split(',')]
            return [col for col in columns if col]  # Remove empty strings
        else:
            # No tuple format, look for column names after 'id,'
            parts = [p.strip() for p in clean_part.split(',')]
            if len(parts) > 1:
                return [parts[1]]  # Return the part after 'id,'
    except Exception as e:
        print(f"Warning: Could not parse metadata columns from '{metadata_part}': {e}")
        return ['SafeName']  # Default fallback
    
    return ['SafeName']  # Default fallback


xml_parser.py

import os
import xml.etree.ElementTree as ET
import json
from collections import OrderedDict
from datetime import date

class XmlParser:
    def __init__(self, xml_file, collection_config):
        self.xml_file = xml_file
        self.collection_config = collection_config

    def parse_xml_file(self):
        try:
            tree = ET.parse(self.xml_file)
            root = tree.getroot()
            namespace = root.tag.split('}')[0].strip('{')
            ns_prefix = f'{{{namespace}}}' if namespace else ''
            collection_element = root.find(f'.//{ns_prefix}{self.collection_config["collections"][0]}')
            if collection_element is None:
                print(f"Collection not found: {self.collection_config['collections'][0]} in {self.xml_file}")
                return None
            data = []
            elements = collection_element.findall(f'{ns_prefix}*')
            attribute_name = self.collection_config["attribute_names"][0]
            attribute_values = self.collection_config["attribute_values"]
            for element in elements:
                if attribute_name in element.attrib and element.attrib.get(attribute_name) in attribute_values:
                    row = OrderedDict()
                    row['id'] = len(data) + 1
                    for attr in element.attrib:
                        row[attr] = element.attrib.get(attr, '')
                    row['ImportDate'] = date.today().strftime("%Y-%m-%d")
                    row['Assignee'] = ''
                    row['Action'] = ''
                    row['Remediation'] = ''
                    row['Comments'] = ''
                    row['Status'] = ""
                    data.append(row)
            return data
        except Exception as e:
            print(f"Error parsing {self.xml_file}: {e}")
            return None

    def parse_xml_file_metadata(self, output_dir, metadata_table_name, table_name):
        try:
            tree = ET.parse(self.xml_file)
            root = tree.getroot()
            namespace = root.tag.split('}')[0].strip('{')
            ns_prefix = f'{{{namespace}}}' if namespace else ''
            collection_element = root.find(f'.//{ns_prefix}{self.collection_config["collections"][0]}')
            if collection_element is None:
                print(f"Collection not found: {self.collection_config['collections'][0]} in {self.xml_file}")
                return None
            
            metadata_rows = []
            today = date.today().strftime("%Y-%m-%d")
            
            # Get custom column names from config
            metadata_columns = self.collection_config.get('metadata_columns', ['SafeName'])
            print(f"Using metadata columns: {metadata_columns}")
            
            # Get elements from XML
            elements = collection_element.findall(f'{ns_prefix}*')
            attribute_name = self.collection_config["attribute_names"][0]
            attribute_values = self.collection_config["attribute_values"]
            
            # Track unique combinations to avoid duplicates
            seen_combinations = set()
            temp_safe_id = 1
            
            for element in elements:
                if attribute_name in element.attrib and element.attrib.get(attribute_name) in attribute_values:
                    # Extract values for the specified metadata columns
                    column_values = []
                    for col_name in metadata_columns:
                        # Look for the column in element attributes (case-insensitive)
                        col_value = None
                        for attr_key in element.attrib:
                            if attr_key.lower() == col_name.lower():
                                col_value = element.attrib.get(attr_key, '')
                                break
                        if col_value is None:
                            col_value = element.attrib.get(col_name, '')  # Fallback to exact match
                        column_values.append(col_value)
                    
                    # Create a unique key from the column values
                    combination_key = tuple(column_values)
                    
                    # Skip if we've already seen this combination today
                    if combination_key in seen_combinations:
                        continue
                    seen_combinations.add(combination_key)
                    
                    metadata_row = OrderedDict()
                    # Don't assign final ID here - let MetadataUpdater handle it
                    metadata_row['id'] = temp_safe_id  # Temporary ID, will be reassigned
                    
                    # Add the custom columns
                    for i, col_name in enumerate(metadata_columns):
                        metadata_row[col_name] = column_values[i]
                    
                    # Calculate SafeID based on the combination of metadata columns
                    # For now, use a simple incremental SafeId - this can be enhanced later
                    metadata_row['SafeId'] = temp_safe_id
                    metadata_row['present_on'] = today
                    metadata_rows.append(metadata_row)
                    temp_safe_id += 1
            
            return metadata_rows
            
        except Exception as e:
            print(f"Error parsing metadata from {self.xml_file}: {e}")
            return None

metadata_updator.py

from db_manager import DBManager
from datetime import date

class MetadataUpdater:
    def __init__(self, metadata_data, db_manager, table_name, collection_config):
        self.metadata_data = metadata_data
        self.db_manager = db_manager
        self.table_name = table_name
        self.collection_config = collection_config

    def update_metadata(self):
        if not self.metadata_data:
            print(f"No metadata to update for table {self.table_name}")
            return
            
        self.db_manager.ensure_table(self.table_name, self.metadata_data[0])
        
        # Get custom column names from config
        metadata_columns = self.collection_config.get('metadata_columns', ['SafeName'])
        
        # Get the maximum ID from the database to continue the sequence
        existing_metadata = self.db_manager.load_metadata(self.table_name)
        max_existing_id = max((row['id'] for row in existing_metadata), default=0)
        
        # Get existing SafeId mappings to maintain consistency
        existing_safe_mappings = self.build_safe_id_mappings(existing_metadata, metadata_columns)
        next_safe_id = max(existing_safe_mappings.values(), default=0) + 1
        
        # Filter new metadata to only include today's date
        today = date.today().strftime("%Y-%m-%d")
        new_metadata_today = [row for row in self.metadata_data if row.get('present_on') == today]
        
        if not new_metadata_today:
            print(f"No new metadata for today ({today}) in table {self.table_name}")
            return
        
        # Assign unique IDs and calculate SafeIds
        for i, row in enumerate(new_metadata_today, start=1):
            row['id'] = max_existing_id + i
            
            # Create a key from the metadata columns for SafeId calculation
            safe_key = self.create_safe_key(row, metadata_columns)
            
            # Assign SafeId - reuse existing or create new
            if safe_key in existing_safe_mappings:
                row['SafeId'] = existing_safe_mappings[safe_key]
            else:
                row['SafeId'] = next_safe_id
                existing_safe_mappings[safe_key] = next_safe_id
                next_safe_id += 1
        
        # Remove duplicates based on custom columns and present_on
        seen_combinations_today = set()
        unique_metadata_today = []
        
        for row in new_metadata_today:
            combination_key = self.create_combination_key(row, metadata_columns)
            if combination_key not in seen_combinations_today:
                seen_combinations_today.add(combination_key)
                unique_metadata_today.append(row)
        
        # Check if we already have entries for today's combinations
        existing_today = [row for row in existing_metadata if row.get('present_on') == today]
        existing_combinations_today = set()
        for row in existing_today:
            existing_combinations_today.add(self.create_combination_key(row, metadata_columns))
        
        # Only insert combinations that don't already exist for today
        final_metadata = []
        for row in unique_metadata_today:
            combination_key = self.create_combination_key(row, metadata_columns)
            if combination_key not in existing_combinations_today:
                final_metadata.append(row)
        
        if final_metadata:
            self.db_manager.insert_rows(self.table_name, final_metadata)
            print(f"Inserted {len(final_metadata)} new metadata rows for {today} into {self.table_name}")
            print(f"Columns used: {metadata_columns}")
        else:
            print(f"No new unique metadata to insert for {today} in table {self.table_name}")

    def build_safe_id_mappings(self, existing_metadata, metadata_columns):
        """Build a mapping of safe keys to SafeIds from existing data"""
        mappings = {}
        for row in existing_metadata:
            safe_key = self.create_safe_key(row, metadata_columns)
            if safe_key and 'SafeId' in row:
                mappings[safe_key] = row['SafeId']
        return mappings

    def create_safe_key(self, row, metadata_columns):
        """Create a unique key from the metadata columns for SafeId calculation"""
        values = []
        for col in metadata_columns:
            values.append(str(row.get(col, '')).lower().strip())
        return tuple(values)

    def create_combination_key(self, row, metadata_columns):
        """Create a unique key for duplicate detection including present_on"""
        values = []
        for col in metadata_columns:
            values.append(str(row.get(col, '')).lower().strip())
        values.append(str(row.get('present_on', '')))
        return tuple(values)

table_processor.py

import os
import re
from datetime import date
from xml_parser import XmlParser
from db_manager import DBManager
from config_loader import load_config
from metadata_updater import MetadataUpdater
from metadata_writer import MetadataWriter

class TableProcessor:
    def __init__(self, xml_dir, output_dir, collection_configs, db_manager):
        self.xml_dir = xml_dir
        self.output_dir = output_dir
        self.collection_configs = collection_configs
        self.db_manager = db_manager
        self.known_prefixes = ["PAM Dashboard Management - "]

    def process_main_table(self):
        recent_files = self.get_most_recent_files()
        for config_name, (file_path,) in recent_files.items():
            print(f"Processing MAIN table file {os.path.basename(file_path)} using config {config_name}")
            xml_parser = XmlParser(file_path, self.collection_configs[config_name])
            data = xml_parser.parse_xml_file()
            if data:
                table_name = f"pam_cyberark_{config_name.replace(' ', '_').lower()}"
                self.db_manager.ensure_table(table_name, data[0])
                self.db_manager.delete_existing_data(table_name)
                self.db_manager.insert_rows(table_name, data)
                print(f"Inserted {len(data)} rows into {table_name}")
                print(f"Data loaded into pgAdmin table: {table_name}")
            else:
                print(f"[INFO] No rows extracted from {file_path}")

    def process_metadata_table(self):
        recent_files = self.get_most_recent_files()
        for config_name, (file_path,) in recent_files.items():
            print(f"Processing file {os.path.basename(file_path)} using config {config_name}")
            table_name = f"pam_cyberark_{config_name.replace(' ', '_').lower()}"
            metadata_table_name = f"pam_cyberark_metadata_{config_name.replace(' ', '_').lower()}"
            
            # Get the collection config for this file
            collection_config = self.collection_configs[config_name]
            
            # Show what columns will be used
            metadata_columns = collection_config.get('metadata_columns', ['SafeName'])
            print(f"Using metadata columns for {config_name}: {metadata_columns}")
            
            xml_parser = XmlParser(file_path, collection_config)
            metadata_data = xml_parser.parse_xml_file_metadata(self.output_dir, metadata_table_name, table_name)
            
            if metadata_data:
                # Pass the collection_config to MetadataUpdater
                metadata_updater = MetadataUpdater(metadata_data, self.db_manager, metadata_table_name, collection_config)
                metadata_updater.update_metadata()
                print(f"Metadata loaded into pgAdmin table: {metadata_table_name}")
            else:
                print(f"[INFO] No metadata extracted from {file_path}")

    def get_most_recent_files(self):
        recent_files = {}
        for root, dirs, files in os.walk(self.xml_dir):
            for file in files:
                if not file.lower().endswith(".xml"):
                    continue
                base_name = os.path.splitext(file)[0]
                base_name = self.strip_known_prefixes(base_name)
                config_name = self.config_name_from_filename(base_name)
                if config_name is None:
                    print(f"[WARN] No config matched for file '{file}'. Skipping.")
                    continue
                full_path = os.path.join(root, file)
                timestamp = os.path.getctime(full_path)
                if config_name not in recent_files or timestamp > recent_files[config_name][1]:
                    recent_files[config_name] = (full_path, timestamp)
        return recent_files

    def strip_known_prefixes(self, name):
        for p in self.known_prefixes:
            if name.startswith(p):
                return name[len(p):].strip()
        return name

    def config_name_from_filename(self, filename_without_ext):
        cleaned = filename_without_ext.replace("_", "").replace("-", "").strip()
        tokens = [t for t in re.split(r"\s+", cleaned) if t]
        for key in self.collection_configs:
            key_norm = key.lower().replace("_", "").strip()
            key_parts = [p for p in re.split(r"\s+|[-]", key_norm) if p]
            if all(part in (t.lower() for t in tokens) for part in key_parts):
                return key
        return None

db_manager.py

import json
import os
from configparser import ConfigParser
from datetime import date
import psycopg2
from psycopg2 import sql
from psycopg2.extras import execute_values, Json

class DBManager:
    def __init__(self, credentials_path="db_credentials.txt"):
        self.creds = self.load_credentials(credentials_path)
        self.conn = self.connect()
        self.schema = self.creds.get("schema", "public")

    def load_credentials(self, path):
        creds = {}
        with open(path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and '=' in line:
                    key, value = line.split('=')
                    creds[key.strip()] = value.strip()
        return creds

    def connect(self):
        return psycopg2.connect(
            host=self.creds["host"],
            port=self.creds["port"],
            dbname=self.creds["database"],
            user=self.creds["user"],
            password=self.creds["password"],
        )

    def ensure_table(self, table_name, sample_row):
        qualified_name = sql.Identifier(self.schema, table_name)
        column_defs = []
        for col, val in sample_row.items():
            pg_type = self.postgres_type(val)
            column_defs.append(sql.SQL("{} {}").format(sql.Identifier(col), sql.SQL(pg_type)))
        column_defs.append(sql.SQL('PRIMARY KEY ("id")'))
        column_defs_sql = sql.SQL(", ").join(column_defs)
        create_table_sql = sql.SQL("CREATE TABLE IF NOT EXISTS {} ({})").format(qualified_name, column_defs_sql)
        with self.conn.cursor() as cur:
            cur.execute(create_table_sql)
            self.conn.commit()

    def postgres_type(self, value):
        if isinstance(value, bool):
            return "BOOLEAN"
        if isinstance(value, int):
            return "INTEGER"
        if isinstance(value, float):
            return "DOUBLE PRECISION"
        return "TEXT"

    def insert_rows(self, table_name, rows):
        if not rows:
            return
        existing_columns = self.get_existing_columns(table_name)
        all_columns = set()
        for r in rows:
            all_columns.update(r.keys())
        column_order = sorted(list(set(existing_columns) & set(all_columns)))
        values = [[row.get(col, None) for col in column_order] for row in rows]
        qualified_name = sql.Identifier(self.schema, table_name)
        cols = sql.SQL(",").join(map(sql.Identifier, column_order))
        update_assignments = sql.SQL(", ").join(sql.SQL('{}=EXCLUDED.{}').format(sql.Identifier(col), sql.Identifier(col)) for col in column_order if col != "id")
        insert_sql = sql.SQL("INSERT INTO {} ({}) VALUES %s ON CONFLICT (id) DO UPDATE SET {}").format(qualified_name, cols, update_assignments)
        with self.conn.cursor() as cur:
            execute_values(cur, insert_sql, values, page_size=100)
            self.conn.commit()

    def get_existing_columns(self, table_name):
        sql_query = """
            SELECT column_name
            FROM information_schema.columns
            WHERE table_schema = %s
            AND table_name = %s;
        """
        with self.conn.cursor() as cur:
            cur.execute(sql_query, (self.schema, table_name))
            return [row[0] for row in cur.fetchall()]

    def delete_existing_data(self, table_name):
        sql_query = sql.SQL("DELETE FROM {}.{}").format(sql.Identifier(self.schema), sql.Identifier(table_name))
        with self.conn.cursor() as cur:
            cur.execute(sql_query)
            self.conn.commit()

    def write_metadata(self, table_name, rows):
        self.ensure_table(table_name, rows[0])
        sql_query = sql.SQL("INSERT INTO {}.{} ({}) VALUES %s").format(sql.Identifier(self.schema), sql.Identifier(table_name), sql.SQL(",").join(map(sql.Identifier, rows[0].keys())))
        values = [[row[col] for col in rows[0].keys()] for row in rows]
        with self.conn.cursor() as cur:
            execute_values(cur, sql_query, values, page_size=100)
            self.conn.commit()

    def load_metadata(self, table_name):
        sql_query = sql.SQL("SELECT * FROM {}.{}").format(sql.Identifier(self.schema), sql.Identifier(table_name))
        with self.conn.cursor() as cur:
            cur.execute(sql_query)
            rows = cur.fetchall()
            columns = [desc[0] for desc in cur.description]
            return [dict(zip(columns, row)) for row in rows]

    def update_metadata(self, table_name, rows):
        self.ensure_table(table_name, rows[0])
        sql_query = sql.SQL("INSERT INTO {}.{} ({}) VALUES %s ON CONFLICT (id) DO UPDATE SET {}").format(
            sql.Identifier(self.schema),
            sql.Identifier(table_name),
            sql.SQL(",").join(map(sql.Identifier, rows[0].keys())),
            sql.SQL(", ").join(sql.SQL('{}=EXCLUDED.{}').format(sql.Identifier(col), sql.Identifier(col)) for col in rows[0].keys() if col != "id")
        )
        values = [[row[col] for col in rows[0].keys()] for row in rows]
        with self.conn.cursor() as cur:
            execute_values(cur, sql_query, values, page_size=100)
            self.conn.commit()

    def close(self):
        self.conn.close()


sample_configfile.txt

xml_dir=C:\XML_Files
output_dir=C:\Output

# Configuration with custom metadata columns
INFORA02=[Details1_Collection], [metier], [CIB ITO IT Client Engagement & Protection IT], [id, ("SERVER_ID", "PAM_LOGINNAME")]
SA09=[Details2_Collection], [Metier2], [CIB ITO IT Client Engagement & Protection IT, CIB ITO IT CIB ITO CCCO], [id, ("SafeName")]

# Traditional configurations (will use default SafeName)
BG01=[Details], [Platform], [Windows Server Local], [id, ("SafeName")]
BG02=[Details], [Platform], [Windows Server Local]
INFUNIX03=[Details], [Platform], [Unix Local]



main.py

import argparse
from config_loader import load_config
from table_processor import TableProcessor
from db_manager import DBManager

def main():
    parser = argparse.ArgumentParser(description='Process XML files.')
    parser.add_argument('--table', choices=['main', 'metadata'], required=True)
    args = parser.parse_args()

    config, collection_configs = load_config('config.txt')

    xml_dir = config.get('xml_dir')
    output_dir = config.get('output_dir')

    if not xml_dir or not output_dir:
        print("Missing xml_dir or output_dir in config.txt")
        return

    db_manager = DBManager()
    table_processor = TableProcessor(xml_dir, output_dir, collection_configs, db_manager)

    if args.table == 'main':
        table_processor.process_main_table()
    elif args.table == 'metadata':
        table_processor.process_metadata_table()

    db_manager.close()

if __name__ == "__main__":
    main()

metadata_writer.py

from db_manager import DBManager

class MetadataWriter:
    def __init__(self, metadata_data, db_manager, table_name):
        self.metadata_data = metadata_data
        self.db_manager = db_manager
        self.table_name = table_name

    def write_metadata(self):
        self.db_manager.write_metadata(self.table_name, self.metadata_data)




















Custom Metadata Columns Implementation
Configuration Format
The configuration file now supports custom metadata column specifications in the format:
CONFIG_NAME=[collection], [attribute_name], [attribute_values], [id, ("COLUMN1", "COLUMN2", ...)]
Examples
Example 1: Multiple Columns (SERVER_ID and PAM_LOGINNAME)
INFORA02=[Details1_Collection], [metier], [CIB ITO IT Client Engagement & Protection IT], [id, ("SERVER_ID", "PAM_LOGINNAME")]
Metadata table for INFORA02 will have columns:

id (auto-generated unique ID)
SERVER_ID (extracted from XML attribute)
PAM_LOGINNAME (extracted from XML attribute)
SafeId (calculated based on SERVER_ID + PAM_LOGINNAME combination)
present_on (date when record was processed)

Example 2: Single Column (SafeName)
SA09=[Details2_Collection], [Metier2], [CIB ITO IT Client Engagement & Protection IT, CIB ITO IT CIB ITO CCCO], [id, ("SafeName")]
Metadata table for SA09 will have columns:

id (auto-generated unique ID)
SafeName (extracted from XML attribute)
SafeId (calculated based on SafeName)
present_on (date when record was processed)

Example 3: Default Behavior (no metadata specification)
BG02=[Details], [Platform], [Windows Server Local]
Metadata table for BG02 will have columns:

id (auto-generated unique ID)
SafeName (default - extracted from XML attribute)
SafeId (calculated based on SafeName)
present_on (date when record was processed)

Sample Data Output
INFORA02 metadata table:
idSERVER_IDPAM_LOGINNAMESafeIdpresent_on1SRV001admin0112024-09-042SRV002service0122024-09-043SRV001admin0112024-09-054SRV003backup0132024-09-05
SA09 metadata table:
idSafeNameSafeIdpresent_on1WindowsSafe112024-09-042UnixSafe122024-09-043WindowsSafe112024-09-054DatabaseSafe32024-09-05
Key Features
1. SafeId Consistency

The SafeId remains consistent for the same combination of metadata columns across different dates
For INFORA02: SERVER_ID="SRV001" + PAM_LOGINNAME="admin01" always gets SafeId=1
For SA09: SafeName="WindowsSafe1" always gets SafeId=1

2. Historical Tracking

Each day's run creates new entries with updated present_on dates
Previous data is never deleted or modified
You can track when specific combinations were present

3. Flexible Column Configuration

Support for single or multiple metadata columns
Case-insensitive attribute matching from XML
Automatic fallback to default "SafeName" if no specification provided

4. Duplicate Prevention

No duplicate combinations for the same date
Unique ID generation prevents database conflicts
Smart deduplication during parsing and insertion

Advanced SafeId Calculation
The SafeId is calculated based on the unique combination of the specified metadata columns:
python# For INFORA02 with ("SERVER_ID", "PAM_LOGINNAME"):
safe_key = (server_id.lower().strip(), pam_loginname.lower().strip())

# For SA09 with ("SafeName"):  
safe_key = (safename.lower().strip(),)
This ensures that:

Same combinations always get the same SafeId
Different combinations get unique SafeIds
The system handles case variations and whitespace

Database Schema Evolution
The metadata tables automatically adapt to the configured columns:
Traditional table (BG02):
sqlCREATE TABLE pam_cyberark_metadata_bg02 (
    id INTEGER PRIMARY KEY,
    SafeName TEXT,
    SafeId INTEGER,
    present_on TEXT
);
Custom multi-column table (INFORA02):
sqlCREATE TABLE pam_cyberark_metadata_infora02 (
    id INTEGER PRIMARY KEY,
    SERVER_ID TEXT,
    PAM_LOGINNAME TEXT,
    SafeId INTEGER,
    present_on TEXT
);
Usage Examples
Query all metadata for a specific server:
sqlSELECT * FROM pam_cyberark_metadata_infora02 
WHERE SERVER_ID = 'SRV001' 
ORDER BY present_on;
Find new safes that appeared on a specific date:
sqlSELECT DISTINCT SafeName FROM pam_cyberark_metadata_sa09 
WHERE present_on = '2024-09-05' 
AND SafeId NOT IN (
    SELECT DISTINCT SafeId FROM pam_cyberark_metadata_sa09 
    WHERE present_on < '2024-09-05'
);
Track login name changes for a server:
sqlSELECT SERVER_ID, PAM_LOGINNAME, present_on 
FROM pam_cyberark_metadata_infora02 
WHERE SERVER_ID = 'SRV001' 
ORDER BY present_on;





Summary
The updated code now supports fully customizable metadata columns through configuration. Here's what the implementation provides:

Flexible Column Configuration: Define any combination of XML attributes as metadata columns
Consistent SafeId Mapping: Same combinations always get the same SafeId across dates
Historical Data Preservation: All previous data remains intact while new entries are appended
Automatic Schema Creation: Database tables adapt to the configured columns
Robust Duplicate Prevention: No conflicts or duplicate entries for the same date

The key changes ensure that your metadata tables can track any combination of attributes (like SERVER_ID + PAM_LOGINNAME) while maintaining referential integrity through consistent SafeId assignments and proper historical tracking.
