# config_loader.py
import ast

def load_config(config_file_path):
    config = {}
    collection_configs = {}
    with open(config_file_path, 'r') as f:
        for line in f:
            line = line.strip()
            if line and '=' in line:
                key, value = line.split('=', 1)
                key = key.strip()
                value = value.strip()
                if key in ['xml_dir', 'output_dir']:
                    config[key] = value
                else:
                    collection_configs[key] = {}
                    parts = [part.strip() for part in value.split(',')]
                    
                    # Parse collections (first part)
                    if len(parts) > 0:
                        collection_configs[key]['collections'] = parts[0].strip().replace('[', '').replace(']', '')
                    
                    # Parse attribute names (second part)
                    if len(parts) > 1:
                        collection_configs[key]['attribute_names'] = parts[1].strip().replace('[', '').replace(']', '')
                    
                    # Parse attribute_values and metadata columns
                    attribute_values = []
                    metadata_columns = None
                    metadata_start_index = None
                    
                    for i, part in enumerate(parts):
                        if 'id,' in part:
                            metadata_start_index = i
                            break
                    
                    if metadata_start_index is not None:
                        # Everything before metadata_start_index is attribute values (except first 2 parts)
                        attribute_parts = parts[2:metadata_start_index]
                        metadata_part = ','.join(parts[metadata_start_index:])
                        metadata_columns = parse_metadata_columns(metadata_part)
                    else:
                        # No metadata specification, all remaining parts are attribute values
                        attribute_parts = parts[2:] if len(parts) > 2 else []
                        metadata_columns = ['SafeName']
                    
                    # Parse attribute values
                    for part in attribute_parts:
                        clean_part = part.strip().replace('[', '').replace(']', '')
                        values = [v.strip() for v in clean_part.split(',') if v.strip()]
                        attribute_values.extend(values)
                    
                    collection_configs[key]['attribute_values'] = attribute_values
                    collection_configs[key]['metadata_columns'] = metadata_columns or ['SafeName']
    
    return config, collection_configs

def parse_metadata_columns(metadata_part):
    """
    Parse metadata column specification from config.
    Examples:
    [id, ("SERVER_ID", "PAM_LOGINNAME")] -> ['SERVER_ID', 'PAM_LOGINNAME']
    [id, (SafeName)] -> ['SafeName']
    """
    try:
        clean_part = metadata_part.strip().replace('[', '').replace(']', '')
        if '(' in clean_part and ')' in clean_part:
            start = clean_part.find('(') + 1
            end = clean_part.find(')')
            tuple_content = clean_part[start:end]
            columns = [col.strip().strip('"').strip("'") for col in tuple_content.split(',')]
            return [col for col in columns if col]
        else:
            parts = [p.strip() for p in clean_part.split(',')]
            if len(parts) > 1:
                return parts[1:]  # Skip 'id' part
    except Exception as e:
        print(f"Warning: Could not parse metadata columns from '{metadata_part}': {e}")
        return ['SafeName']
    return ['SafeName']


# xml_parser.py
import os
import xml.etree.ElementTree as ET
import json
from collections import OrderedDict
from datetime import date

class XmlParser:
    def __init__(self, xml_file, collection_config):
        self.xml_file = xml_file
        self.collection_config = collection_config

    def parse_xml_file(self):
        try:
            tree = ET.parse(self.xml_file)
            root = tree.getroot()
            
            # Extract namespace properly
            namespace = ""
            if root.tag.startswith('{'):
                namespace = root.tag.split('}')[0].strip('{')
            
            ns_prefix = f'{{{namespace}}}' if namespace else ""
            
            # Find collection element
            collection_name = self.collection_config["collections"]
            collection_element = root.find(f'.//{ns_prefix}{collection_name}')
            
            if collection_element is None:
                print(f"Collection not found: {collection_name} in {self.xml_file}")
                return None
            
            data = []
            elements = collection_element.findall(f'{ns_prefix}*')  # Fixed: added wildcard
            attribute_name = self.collection_config["attribute_names"]
            attribute_values = self.collection_config["attribute_values"]
            
            for element in elements:
                if attribute_name in element.attrib and element.attrib.get(attribute_name) in attribute_values:
                    row = OrderedDict()
                    row['id'] = len(data) + 1
                    
                    # Add all element attributes
                    for attr in element.attrib:
                        row[attr] = element.attrib.get(attr, '')
                    
                    # Add standard fields
                    row['ImportDate'] = date.today().strftime("%Y-%m-%d")
                    row['Assignee'] = ''
                    row['Action'] = ''
                    row['Remediation'] = ''
                    row['Comments'] = ''
                    row['Status'] = ''
                    
                    data.append(row)
            
            return data
            
        except Exception as e:
            print(f"Error parsing {self.xml_file}: {e}")
            return None

    def parse_xml_file_metadata(self, output_dir, metadata_table_name, table_name):
        try:
            tree = ET.parse(self.xml_file)
            root = tree.getroot()
            
            # Extract namespace properly
            namespace = ""
            if root.tag.startswith('{'):
                namespace = root.tag.split('}')[0].strip('{')
            
            ns_prefix = f'{{{namespace}}}' if namespace else ""
            
            # Find collection element
            collection_name = self.collection_config["collections"]
            collection_element = root.find(f'.//{ns_prefix}{collection_name}')
            
            if collection_element is None:
                print(f"Collection not found: {collection_name} in {self.xml_file}")
                return None
            
            metadata_rows = []
            today = date.today().strftime("%Y-%m-%d")
            metadata_columns = self.collection_config.get('metadata_columns', ['SafeName'])
            print(f"Using metadata columns: {metadata_columns}")
            
            elements = collection_element.findall(f'{ns_prefix}*')  # Fixed: added wildcard
            attribute_name = self.collection_config["attribute_names"]
            attribute_values = self.collection_config["attribute_values"]
            
            seen_combinations = set()
            temp_safe_id = 1
            
            for element in elements:
                if attribute_name in element.attrib and element.attrib.get(attribute_name) in attribute_values:
                    # Extract values for each metadata column
                    column_values = []
                    for col_name in metadata_columns:
                        col_value = None
                        
                        # Case-insensitive attribute lookup
                        for attr_key in element.attrib:
                            if attr_key.lower() == col_name.lower():
                                col_value = element.attrib.get(attr_key, '')
                                break
                        
                        # Fallback to exact match if case-insensitive didn't work
                        if col_value is None:
                            col_value = element.attrib.get(col_name, '')
                        
                        column_values.append(col_value)
                    
                    # Create combination key to avoid duplicates
                    combination_key = tuple(column_values)
                    
                    # Skip if we've already seen this combination
                    if combination_key in seen_combinations:
                        continue
                    
                    # Add to seen combinations
                    seen_combinations.add(combination_key)
                    
                    # Create metadata row
                    metadata_row = OrderedDict()
                    metadata_row['id'] = temp_safe_id
                    
                    # Add each metadata column value
                    for i, col_name in enumerate(metadata_columns):
                        metadata_row[col_name] = column_values[i]
                    
                    # Add standard metadata fields
                    metadata_row['SafeId'] = temp_safe_id
                    metadata_row['present_on'] = today
                    
                    metadata_rows.append(metadata_row)
                    temp_safe_id += 1
            
            return metadata_rows
            
        except Exception as e:
            print(f"Error parsing metadata from {self.xml_file}: {e}")
            return None


# db_manager.py
import json
import os
from configparser import ConfigParser
from datetime import date
import psycopg2
from psycopg2 import sql
from psycopg2.extras import execute_values, Json

class DBManager:
    def __init__(self, credentials_path: str = "db_credentials.txt"):
        self.creds = self.load_credentials(credentials_path)
        self.conn = self.connect()
        self.schema = self.creds.get("schema", "public")

    @staticmethod
    def load_credentials(path: str) -> dict:
        creds = {}
        with open(path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and '=' in line:
                    key, value = line.split('=', 1)  # Fixed: limit split to 1
                    creds[key.strip()] = value.strip()
        return creds

    def connect(self):
        print(f"Connecting to host: {self.creds['host']}")
        return psycopg2.connect(
            host=self.creds["host"],
            port=self.creds["port"],
            dbname=self.creds["database"],
            user=self.creds["user"],
            password=self.creds["password"],
        )

    def ensure_table(self, table_name: str, sample_row: dict):
        if not sample_row:  # Handle empty sample_row
            return
            
        qualified_name = sql.Identifier(self.schema, table_name)
        column_defs = []
        
        for col, val in sample_row.items():
            pg_type = self.postgres_type(val)
            column_defs.append(sql.SQL("{} {}").format(sql.Identifier(col), sql.SQL(pg_type)))
        
        column_defs.append(sql.SQL('PRIMARY KEY ("id")'))
        column_defs_sql = sql.SQL(", ").join(column_defs)
        
        create_table_sql = sql.SQL(
            "CREATE TABLE IF NOT EXISTS {} ({})"
        ).format(qualified_name, column_defs_sql)
        
        with self.conn.cursor() as cur:
            cur.execute(create_table_sql)
            self.conn.commit()
        
        # Add missing columns
        existing_columns = self.get_existing_columns(table_name)
        missing = set(sample_row.keys()) - set(existing_columns)
        
        if missing:
            alter_stmts = []
            for col in missing:
                col_type = self.postgres_type(sample_row[col])
                stmt = sql.SQL(
                    "ALTER TABLE {} ADD COLUMN {} {}"
                ).format(qualified_name, sql.Identifier(col), sql.SQL(col_type))
                alter_stmts.append(stmt)
            
            with self.conn.cursor() as cur:
                for stmt in alter_stmts:
                    cur.execute(stmt)
                self.conn.commit()

    def get_existing_columns(self, table_name: str):
        sql_query = """
        SELECT column_name FROM information_schema.columns 
        WHERE table_schema=%s AND table_name=%s;
        """
        with self.conn.cursor() as cur:
            cur.execute(sql_query, (self.schema, table_name))
            return [row[0] for row in cur.fetchall()]  # Fixed: extract column name

    def delete_existing_data(self, table_name: str):
        # Check if table exists
        sql_query = sql.SQL("""
        SELECT EXISTS (
            SELECT 1
            FROM information_schema.tables
            WHERE table_schema = %s AND table_name = %s
        )
        """)
        
        with self.conn.cursor() as cur:
            cur.execute(sql_query, (self.schema, table_name))
            exists = cur.fetchone()[0]  # Fixed: extract boolean value
        
        if exists:
            sql_query = sql.SQL("DELETE FROM {}.{}").format(
                sql.Identifier(self.schema), 
                sql.Identifier(table_name)
            )
            with self.conn.cursor() as cur:
                cur.execute(sql_query)
                self.conn.commit()

    def insert_rows(self, table_name: str, rows: list):
        if not rows:
            return
        
        # Ensure table exists with first row as sample
        self.ensure_table(table_name, rows[0])
        
        existing_columns = self.get_existing_columns(table_name)
        all_columns = set()
        for r in rows:
            all_columns.update(r.keys())
        
        column_order = sorted(list(set(existing_columns) & set(all_columns)))
        
        values = [
            [row.get(col, None) for col in column_order] for row in rows
        ]
        
        qualified_name = sql.Identifier(self.schema, table_name)
        cols_sql = sql.SQL(",").join(map(sql.Identifier, column_order))
        
        update_assignments = sql.SQL(",").join(
            sql.SQL('{}=EXCLUDED.{}').format(sql.Identifier(col), sql.Identifier(col))
            for col in column_order if col != "id"
        )
        
        insert_sql = sql.SQL(
            "INSERT INTO {} ({}) VALUES %s ON CONFLICT (id) DO UPDATE SET {}"
        ).format(
            qualified_name, cols_sql, update_assignments,
        )
        
        with self.conn.cursor() as cur:
            execute_values(cur, insert_sql, values, page_size=100)
            self.conn.commit()

    def write_metadata(self, table_name: str, rows: list):
        if not rows:
            return
            
        self.ensure_table(table_name, rows[0])
        
        column_order = list(rows[0].keys())
        qualified_name = sql.Identifier(self.schema, table_name)
        cols_sql = sql.SQL(",").join(map(sql.Identifier, column_order))
        
        sql_query = sql.SQL("""
        INSERT INTO {} ({}) VALUES %s
        """).format(qualified_name, cols_sql)
        
        values = [[row[col] for col in column_order] for row in rows]
        
        with self.conn.cursor() as cur:
            execute_values(cur, sql_query, values, page_size=100)
            self.conn.commit()

    def load_metadata(self, table_name: str):
        # Check if table exists first
        check_sql = sql.SQL("""
        SELECT EXISTS (
            SELECT 1 FROM information_schema.tables 
            WHERE table_schema = %s AND table_name = %s
        )
        """)
        
        with self.conn.cursor() as cur:
            cur.execute(check_sql, (self.schema, table_name))
            exists = cur.fetchone()[0]
        
        if not exists:
            return []
        
        sql_query = sql.SQL("""
        SELECT * FROM {}.{}
        """).format(
            sql.Identifier(self.schema),
            sql.Identifier(table_name)
        )
        
        with self.conn.cursor() as cur:
            cur.execute(sql_query)
            rows = cur.fetchall()
            columns = [desc[0] for desc in cur.description]  # Fixed: extract column name
            return [dict(zip(columns, row)) for row in rows]

    def update_metadata(self, table_name: str, rows: list):
        if not rows:
            return
            
        self.ensure_table(table_name, rows[0])
        
        column_order = list(rows[0].keys())
        qualified_name = sql.Identifier(self.schema, table_name)
        cols_sql = sql.SQL(",").join(map(sql.Identifier, column_order))
        
        update_assignments = sql.SQL(", ").join(
            sql.SQL('{}=EXCLUDED.{}').format(sql.Identifier(col), sql.Identifier(col))
            for col in column_order if col != "id"
        )
        
        sql_query = sql.SQL("""
        INSERT INTO {} ({}) VALUES %s ON CONFLICT (id) DO UPDATE SET {}
        """).format(qualified_name, cols_sql, update_assignments)
        
        values = [[row[col] for col in column_order] for row in rows]
        
        with self.conn.cursor() as cur:
            execute_values(cur, sql_query, values, page_size=100)
            self.conn.commit()

    @staticmethod
    def postgres_type(value):
        if isinstance(value, bool):
            return "BOOLEAN"
        if isinstance(value, int):
            return "INTEGER"
        if isinstance(value, float):
            return "DOUBLE PRECISION"
        return "TEXT"

    def close(self):
        if self.conn:
            self.conn.close()


# metadata_updater.py
from db_manager import DBManager
from datetime import date

class MetadataUpdater:
    def __init__(self, metadata_data, db_manager, table_name, collection_config):
        self.metadata_data = metadata_data
        self.db_manager = db_manager
        self.table_name = table_name
        self.collection_config = collection_config

    def update_metadata(self):
        if not self.metadata_data:
            print(f"No metadata to update for table {self.table_name}")
            return
        
        # Ensure table exists
        self.db_manager.ensure_table(self.table_name, self.metadata_data[0])
        
        # Load existing metadata
        existing_metadata = self.db_manager.load_metadata(self.table_name)
        
        # Get max existing ID
        max_existing_id = max((row['id'] for row in existing_metadata), default=0)
        
        today = date.today().strftime("%Y-%m-%d")
        
        # Filter for today's data
        new_metadata_today = [row for row in self.metadata_data if row.get('present_on') == today]
        
        if not new_metadata_today:
            print(f"No new metadata for today ({today}) in table {self.table_name}")
            return
        
        # Update IDs for new metadata
        for i, row in enumerate(new_metadata_today, start=1):
            row['id'] = max_existing_id + i
        
        # Remove duplicates within new data
        seen_safes_today = set()
        unique_metadata_today = []
        
        for row in new_metadata_today:
            # Create key based on all metadata columns, not just SafeName
            key_values = []
            for col in self.collection_config.get('metadata_columns', ['SafeName']):
                key_values.append(row.get(col, ''))
            safe_key = tuple(key_values + [row.get('present_on', '')])
            
            if safe_key not in seen_safes_today:
                seen_safes_today.add(safe_key)
                unique_metadata_today.append(row)
        
        # Check against existing data for today
        existing_today = [row for row in existing_metadata if row.get('present_on') == today]
        existing_safes_today = set()
        
        for row in existing_today:
            key_values = []
            for col in self.collection_config.get('metadata_columns', ['SafeName']):
                key_values.append(row.get(col, ''))
            existing_safes_today.add(tuple(key_values + [row.get('present_on', '')]))
        
        # Final filtering - only include truly new entries
        final_metadata = []
        for row in unique_metadata_today:
            key_values = []
            for col in self.collection_config.get('metadata_columns', ['SafeName']):
                key_values.append(row.get(col, ''))
            safe_key = tuple(key_values + [row.get('present_on', '')])
            
            if safe_key not in existing_safes_today:
                final_metadata.append(row)
        
        if final_metadata:
            self.db_manager.insert_rows(self.table_name, final_metadata)
            print(f"Inserted {len(final_metadata)} new metadata rows for {today} into {self.table_name}")
        else:
            print(f"No new unique metadata to insert for {today} in table {self.table_name}")


# metadata_writer.py
from db_manager import DBManager

class MetadataWriter:
    def __init__(self, metadata_data, db_manager, table_name):
        self.metadata_data = metadata_data
        self.db_manager = db_manager
        self.table_name = table_name

    def write_metadata(self):
        if self.metadata_data:
            self.db_manager.write_metadata(self.table_name, self.metadata_data)
            print(f"Metadata written to {self.table_name}")
        else:
            print("No metadata to write.")


# table_processor.py
import os
import re
from datetime import date
from xml_parser import XmlParser
from db_manager import DBManager
from config_loader import load_config
from metadata_updater import MetadataUpdater
from metadata_writer import MetadataWriter

class TableProcessor:
    def __init__(self, xml_dir, output_dir, collection_configs, db_manager):
        self.xml_dir = xml_dir
        self.output_dir = output_dir
        self.collection_configs = collection_configs
        self.db_manager = db_manager
        self.known_prefixes = ["PAM Dashboard Management"]

    def process_main_table(self):
        recent_files = self.get_most_recent_files()
        
        for config_name, (file_path, timestamp) in recent_files.items():
            print(f"Processing MAIN table file {os.path.basename(file_path)} using config {config_name}")
            
            xml_parser = XmlParser(file_path, self.collection_configs[config_name])
            data = xml_parser.parse_xml_file()
            
            if data:
                table_name = f"pam_cyberark_{config_name.replace(' ', '').lower()}"
                self.db_manager.ensure_table(table_name, data[0])
                self.db_manager.delete_existing_data(table_name)
                self.db_manager.insert_rows(table_name, data)
                print(f"Inserted {len(data)} rows into {table_name}")
                print(f"Data loaded into pgAdmin table: {table_name}")
            else:
                print(f"[INFO] No rows extracted from {file_path}")

    def process_metadata_table(self):
        recent_files = self.get_most_recent_files()
        
        for config_name, (file_path, timestamp) in recent_files.items():
            print(f"Processing file {os.path.basename(file_path)} using config {config_name}")
            
            table_name = f"pam_cyberark_{config_name.replace(' ', '').lower()}"
            metadata_table_name = f"pam_cyberark_metadata_{config_name.replace(' ', '').lower()}"
            
            collection_config = self.collection_configs[config_name]
            metadata_columns = collection_config.get('metadata_columns', ['SafeName'])
            print(f"Using metadata columns for {config_name}: {metadata_columns}")
            
            xml_parser = XmlParser(file_path, collection_config)
            metadata_data = xml_parser.parse_xml_file_metadata(
                self.output_dir, metadata_table_name, table_name
            )
            
            if metadata_data:
                metadata_updater = MetadataUpdater(
                    metadata_data, self.db_manager, metadata_table_name, collection_config
                )
                metadata_updater.update_metadata()
                print(f"Metadata loaded into pgAdmin table: {metadata_table_name}")
            else:
                print(f"[INFO] No metadata extracted from {file_path}")

    def get_most_recent_files(self):
        recent_files = {}
        
        for root, dirs, files in os.walk(self.xml_dir):
            for file in files:
                if not file.lower().endswith(".xml"):
                    continue
                
                base_name = os.path.splitext(file)[0]  # Fixed: extract filename without extension
                base_name = self.strip_known_prefixes(base_name)
                config_name = self.config_name_from_filename(base_name)
                
                if config_name is None:
                    print(f"[WARN] No config matched for file {file}. Skipping")
                    continue
                
                full_path = os.path.join(root, file)
                timestamp = os.path.getctime(full_path)
                
                if config_name not in recent_files or timestamp > recent_files[config_name][1]:
                    recent_files[config_name] = (full_path, timestamp)
        
        return recent_files

    def strip_known_prefixes(self, name):
        for p in self.known_prefixes:
            if name.startswith(p):
                return name[len(p):].strip()
        return name

    def config_name_from_filename(self, filename_without_ext):
        cleaned = filename_without_ext.replace(' ', '').replace('-', '').strip()
        tokens = [t for t in re.split(r"\s+", cleaned) if t]
        
        for key in self.collection_configs:
            key_norm = key.lower().replace(' ', '').strip()
            key_parts = [p for p in re.split(r"\s+|[-]", key_norm) if p]
            
            if all(part in (t.lower() for t in tokens) for part in key_parts):
                return key
        
        return None


# main.py
import argparse
from config_loader import load_config
from db_manager import DBManager
from table_processor import TableProcessor

def main():
    parser = argparse.ArgumentParser(description='Process XML files.')
    parser.add_argument('--table', choices=['main', 'metadata'], required=True,
                       help='Type of table to process')
    args = parser.parse_args()
    
    # Load configuration
    config, collection_configs = load_config('config.txt')
    xml_dir = config.get('xml_dir')
    output_dir = config.get('output_dir')
    
    if not xml_dir or not output_dir:
        print("Missing xml_dir or output_dir in config.txt")
        return
    
    # Initialize database manager
    db_manager = DBManager()
    
    # Initialize table processor
    table_processor = TableProcessor(xml_dir, output_dir, collection_configs, db_manager)
    
    # Process based on argument
    if args.table == 'main':
        table_processor.process_main_table()
    elif args.table == 'metadata':
        table_processor.process_metadata_table()
    
    # Clean up
    db_manager.close()

if __name__ == "__main__":
    main()































































































config_loader.py

import ast

def load_config(config_file_path):
    config = {}
    collection_configs = {}
    with open(config_file_path, 'r') as f:
        for line in f:
            line = line.strip()
            if line and '=' in line:
                key, value = line.split('=', 1)  # Split only on first '=' to handle complex values
                key = key.strip()
                value = value.strip()
                if key in ['xml_dir', 'output_dir']:
                    config[key] = value
                else:
                    collection_configs[key] = {}
                    parts = [part.strip() for part in value.split(',')]
                    
                    # Parse collections
                    collection_configs[key]['collections'] = [parts[0].strip().replace('[', '').replace(']', '')]
                    
                    # Parse attribute_names
                    collection_configs[key]['attribute_names'] = [parts[1].strip().replace('[', '').replace(']', '')]
                    
                    # Parse attribute_values (can be multiple)
                    attribute_values = []
                    metadata_columns = None
                    
                    # Look for metadata column specification in parts[3] if it exists
                    for i, part in enumerate(parts[2:], start=2):
                        # Check if this part contains metadata column specification
                        if i == 3 and '[id,' in part:
                            # This is the metadata column specification
                            # Extract the metadata columns part
                            metadata_part = part
                            # Parse the metadata column specification
                            metadata_columns = parse_metadata_columns(metadata_part)
                        else:
                            # This is an attribute value
                            if not ('[id,' in part):  # Skip metadata specification parts
                                attribute_values.extend([p.strip() for p in part.split(',')])
                    
                    # If no attribute values found in parts[2], parse it normally
                    if not attribute_values and len(parts) > 2:
                        if not ('[id,' in parts[2]):
                            attribute_values.extend([p.strip() for p in parts[2].split(',')])
                    
                    collection_configs[key]['attribute_values'] = attribute_values
                    collection_configs[key]['metadata_columns'] = metadata_columns or ['SafeName']  # Default
                    
    return config, collection_configs

def parse_metadata_columns(metadata_part):
    """
    Parse metadata column specification from config.
    Examples:
    - [id, ("SERVER_ID", "PAM_LOGINNAME")] -> ['SERVER_ID', 'PAM_LOGINNAME']
    - [id, (SafeName)] -> ['SafeName']
    """
    try:
        # Remove outer brackets and split by comma
        clean_part = metadata_part.strip().replace('[', '').replace(']', '')
        
        # Find the tuple part
        if '(' in clean_part and ')' in clean_part:
            # Extract content between parentheses
            start = clean_part.find('(') + 1
            end = clean_part.find(')')
            tuple_content = clean_part[start:end]
            
            # Split by comma and clean up
            columns = [col.strip().strip('"').strip("'") for col in tuple_content.split(',')]
            return [col for col in columns if col]  # Remove empty strings
        else:
            # No tuple format, look for column names after 'id,'
            parts = [p.strip() for p in clean_part.split(',')]
            if len(parts) > 1:
                return [parts[1]]  # Return the part after 'id,'
    except Exception as e:
        print(f"Warning: Could not parse metadata columns from '{metadata_part}': {e}")
        return ['SafeName']  # Default fallback
    
    return ['SafeName']  # Default fallback


xml_parser.py

import os
import xml.etree.ElementTree as ET
import json
from collections import OrderedDict
from datetime import date

class XmlParser:
    def __init__(self, xml_file, collection_config):
        self.xml_file = xml_file
        self.collection_config = collection_config

    def parse_xml_file(self):
        try:
            tree = ET.parse(self.xml_file)
            root = tree.getroot()
            namespace = root.tag.split('}')[0].strip('{')
            ns_prefix = f'{{{namespace}}}' if namespace else ''
            collection_element = root.find(f'.//{ns_prefix}{self.collection_config["collections"][0]}')
            if collection_element is None:
                print(f"Collection not found: {self.collection_config['collections'][0]} in {self.xml_file}")
                return None
            data = []
            elements = collection_element.findall(f'{ns_prefix}*')
            attribute_name = self.collection_config["attribute_names"][0]
            attribute_values = self.collection_config["attribute_values"]
            for element in elements:
                if attribute_name in element.attrib and element.attrib.get(attribute_name) in attribute_values:
                    row = OrderedDict()
                    row['id'] = len(data) + 1
                    for attr in element.attrib:
                        row[attr] = element.attrib.get(attr, '')
                    row['ImportDate'] = date.today().strftime("%Y-%m-%d")
                    row['Assignee'] = ''
                    row['Action'] = ''
                    row['Remediation'] = ''
                    row['Comments'] = ''
                    row['Status'] = ""
                    data.append(row)
            return data
        except Exception as e:
            print(f"Error parsing {self.xml_file}: {e}")
            return None

    def parse_xml_file_metadata(self, output_dir, metadata_table_name, table_name):
        try:
            tree = ET.parse(self.xml_file)
            root = tree.getroot()
            namespace = root.tag.split('}')[0].strip('{')
            ns_prefix = f'{{{namespace}}}' if namespace else ''
            collection_element = root.find(f'.//{ns_prefix}{self.collection_config["collections"][0]}')
            if collection_element is None:
                print(f"Collection not found: {self.collection_config['collections'][0]} in {self.xml_file}")
                return None
            
            metadata_rows = []
            today = date.today().strftime("%Y-%m-%d")
            
            # Get custom column names from config
            metadata_columns = self.collection_config.get('metadata_columns', ['SafeName'])
            print(f"Using metadata columns: {metadata_columns}")
            
            # Get elements from XML
            elements = collection_element.findall(f'{ns_prefix}*')
            attribute_name = self.collection_config["attribute_names"][0]
            attribute_values = self.collection_config["attribute_values"]
            
            # Track unique combinations to avoid duplicates
            seen_combinations = set()
            temp_safe_id = 1
            
            for element in elements:
                if attribute_name in element.attrib and element.attrib.get(attribute_name) in attribute_values:
                    # Extract values for the specified metadata columns
                    column_values = []
                    for col_name in metadata_columns:
                        # Look for the column in element attributes (case-insensitive)
                        col_value = None
                        for attr_key in element.attrib:
                            if attr_key.lower() == col_name.lower():
                                col_value = element.attrib.get(attr_key, '')
                                break
                        if col_value is None:
                            col_value = element.attrib.get(col_name, '')  # Fallback to exact match
                        column_values.append(col_value)
                    
                    # Create a unique key from the column values
                    combination_key = tuple(column_values)
                    
                    # Skip if we've already seen this combination today
                    if combination_key in seen_combinations:
                        continue
                    seen_combinations.add(combination_key)
                    
                    metadata_row = OrderedDict()
                    # Don't assign final ID here - let MetadataUpdater handle it
                    metadata_row['id'] = temp_safe_id  # Temporary ID, will be reassigned
                    
                    # Add the custom columns
                    for i, col_name in enumerate(metadata_columns):
                        metadata_row[col_name] = column_values[i]
                    
                    # Calculate SafeID based on the combination of metadata columns
                    # For now, use a simple incremental SafeId - this can be enhanced later
                    metadata_row['SafeId'] = temp_safe_id
                    metadata_row['present_on'] = today
                    metadata_rows.append(metadata_row)
                    temp_safe_id += 1
            
            return metadata_rows
            
        except Exception as e:
            print(f"Error parsing metadata from {self.xml_file}: {e}")
            return None

metadata_updator.py

from db_manager import DBManager
from datetime import date

class MetadataUpdater:
    def __init__(self, metadata_data, db_manager, table_name, collection_config):
        self.metadata_data = metadata_data
        self.db_manager = db_manager
        self.table_name = table_name
        self.collection_config = collection_config

    def update_metadata(self):
        if not self.metadata_data:
            print(f"No metadata to update for table {self.table_name}")
            return
            
        self.db_manager.ensure_table(self.table_name, self.metadata_data[0])
        
        # Get custom column names from config
        metadata_columns = self.collection_config.get('metadata_columns', ['SafeName'])
        
        # Get the maximum ID from the database to continue the sequence
        existing_metadata = self.db_manager.load_metadata(self.table_name)
        max_existing_id = max((row['id'] for row in existing_metadata), default=0)
        
        # Get existing SafeId mappings to maintain consistency
        existing_safe_mappings = self.build_safe_id_mappings(existing_metadata, metadata_columns)
        next_safe_id = max(existing_safe_mappings.values(), default=0) + 1
        
        # Filter new metadata to only include today's date
        today = date.today().strftime("%Y-%m-%d")
        new_metadata_today = [row for row in self.metadata_data if row.get('present_on') == today]
        
        if not new_metadata_today:
            print(f"No new metadata for today ({today}) in table {self.table_name}")
            return
        
        # Assign unique IDs and calculate SafeIds
        for i, row in enumerate(new_metadata_today, start=1):
            row['id'] = max_existing_id + i
            
            # Create a key from the metadata columns for SafeId calculation
            safe_key = self.create_safe_key(row, metadata_columns)
            
            # Assign SafeId - reuse existing or create new
            if safe_key in existing_safe_mappings:
                row['SafeId'] = existing_safe_mappings[safe_key]
            else:
                row['SafeId'] = next_safe_id
                existing_safe_mappings[safe_key] = next_safe_id
                next_safe_id += 1
        
        # Remove duplicates based on custom columns and present_on
        seen_combinations_today = set()
        unique_metadata_today = []
        
        for row in new_metadata_today:
            combination_key = self.create_combination_key(row, metadata_columns)
            if combination_key not in seen_combinations_today:
                seen_combinations_today.add(combination_key)
                unique_metadata_today.append(row)
        
        # Check if we already have entries for today's combinations
        existing_today = [row for row in existing_metadata if row.get('present_on') == today]
        existing_combinations_today = set()
        for row in existing_today:
            existing_combinations_today.add(self.create_combination_key(row, metadata_columns))
        
        # Only insert combinations that don't already exist for today
        final_metadata = []
        for row in unique_metadata_today:
            combination_key = self.create_combination_key(row, metadata_columns)
            if combination_key not in existing_combinations_today:
                final_metadata.append(row)
        
        if final_metadata:
            self.db_manager.insert_rows(self.table_name, final_metadata)
            print(f"Inserted {len(final_metadata)} new metadata rows for {today} into {self.table_name}")
            print(f"Columns used: {metadata_columns}")
        else:
            print(f"No new unique metadata to insert for {today} in table {self.table_name}")

    def build_safe_id_mappings(self, existing_metadata, metadata_columns):
        """Build a mapping of safe keys to SafeIds from existing data"""
        mappings = {}
        for row in existing_metadata:
            safe_key = self.create_safe_key(row, metadata_columns)
            if safe_key and 'SafeId' in row:
                mappings[safe_key] = row['SafeId']
        return mappings

    def create_safe_key(self, row, metadata_columns):
        """Create a unique key from the metadata columns for SafeId calculation"""
        values = []
        for col in metadata_columns:
            values.append(str(row.get(col, '')).lower().strip())
        return tuple(values)

    def create_combination_key(self, row, metadata_columns):
        """Create a unique key for duplicate detection including present_on"""
        values = []
        for col in metadata_columns:
            values.append(str(row.get(col, '')).lower().strip())
        values.append(str(row.get('present_on', '')))
        return tuple(values)

table_processor.py

import os
import re
from datetime import date
from xml_parser import XmlParser
from db_manager import DBManager
from config_loader import load_config
from metadata_updater import MetadataUpdater
from metadata_writer import MetadataWriter

class TableProcessor:
    def __init__(self, xml_dir, output_dir, collection_configs, db_manager):
        self.xml_dir = xml_dir
        self.output_dir = output_dir
        self.collection_configs = collection_configs
        self.db_manager = db_manager
        self.known_prefixes = ["PAM Dashboard Management - "]

    def process_main_table(self):
        recent_files = self.get_most_recent_files()
        for config_name, (file_path,) in recent_files.items():
            print(f"Processing MAIN table file {os.path.basename(file_path)} using config {config_name}")
            xml_parser = XmlParser(file_path, self.collection_configs[config_name])
            data = xml_parser.parse_xml_file()
            if data:
                table_name = f"pam_cyberark_{config_name.replace(' ', '_').lower()}"
                self.db_manager.ensure_table(table_name, data[0])
                self.db_manager.delete_existing_data(table_name)
                self.db_manager.insert_rows(table_name, data)
                print(f"Inserted {len(data)} rows into {table_name}")
                print(f"Data loaded into pgAdmin table: {table_name}")
            else:
                print(f"[INFO] No rows extracted from {file_path}")

    def process_metadata_table(self):
        recent_files = self.get_most_recent_files()
        for config_name, (file_path,) in recent_files.items():
            print(f"Processing file {os.path.basename(file_path)} using config {config_name}")
            table_name = f"pam_cyberark_{config_name.replace(' ', '_').lower()}"
            metadata_table_name = f"pam_cyberark_metadata_{config_name.replace(' ', '_').lower()}"
            
            # Get the collection config for this file
            collection_config = self.collection_configs[config_name]
            
            # Show what columns will be used
            metadata_columns = collection_config.get('metadata_columns', ['SafeName'])
            print(f"Using metadata columns for {config_name}: {metadata_columns}")
            
            xml_parser = XmlParser(file_path, collection_config)
            metadata_data = xml_parser.parse_xml_file_metadata(self.output_dir, metadata_table_name, table_name)
            
            if metadata_data:
                # Pass the collection_config to MetadataUpdater
                metadata_updater = MetadataUpdater(metadata_data, self.db_manager, metadata_table_name, collection_config)
                metadata_updater.update_metadata()
                print(f"Metadata loaded into pgAdmin table: {metadata_table_name}")
            else:
                print(f"[INFO] No metadata extracted from {file_path}")

    def get_most_recent_files(self):
        recent_files = {}
        for root, dirs, files in os.walk(self.xml_dir):
            for file in files:
                if not file.lower().endswith(".xml"):
                    continue
                base_name = os.path.splitext(file)[0]
                base_name = self.strip_known_prefixes(base_name)
                config_name = self.config_name_from_filename(base_name)
                if config_name is None:
                    print(f"[WARN] No config matched for file '{file}'. Skipping.")
                    continue
                full_path = os.path.join(root, file)
                timestamp = os.path.getctime(full_path)
                if config_name not in recent_files or timestamp > recent_files[config_name][1]:
                    recent_files[config_name] = (full_path, timestamp)
        return recent_files

    def strip_known_prefixes(self, name):
        for p in self.known_prefixes:
            if name.startswith(p):
                return name[len(p):].strip()
        return name

    def config_name_from_filename(self, filename_without_ext):
        cleaned = filename_without_ext.replace("_", "").replace("-", "").strip()
        tokens = [t for t in re.split(r"\s+", cleaned) if t]
        for key in self.collection_configs:
            key_norm = key.lower().replace("_", "").strip()
            key_parts = [p for p in re.split(r"\s+|[-]", key_norm) if p]
            if all(part in (t.lower() for t in tokens) for part in key_parts):
                return key
        return None

db_manager.py

import json
import os
from configparser import ConfigParser
from datetime import date
import psycopg2
from psycopg2 import sql
from psycopg2.extras import execute_values, Json

class DBManager:
    def __init__(self, credentials_path="db_credentials.txt"):
        self.creds = self.load_credentials(credentials_path)
        self.conn = self.connect()
        self.schema = self.creds.get("schema", "public")

    def load_credentials(self, path):
        creds = {}
        with open(path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and '=' in line:
                    key, value = line.split('=')
                    creds[key.strip()] = value.strip()
        return creds

    def connect(self):
        return psycopg2.connect(
            host=self.creds["host"],
            port=self.creds["port"],
            dbname=self.creds["database"],
            user=self.creds["user"],
            password=self.creds["password"],
        )

    def ensure_table(self, table_name, sample_row):
        qualified_name = sql.Identifier(self.schema, table_name)
        column_defs = []
        for col, val in sample_row.items():
            pg_type = self.postgres_type(val)
            column_defs.append(sql.SQL("{} {}").format(sql.Identifier(col), sql.SQL(pg_type)))
        column_defs.append(sql.SQL('PRIMARY KEY ("id")'))
        column_defs_sql = sql.SQL(", ").join(column_defs)
        create_table_sql = sql.SQL("CREATE TABLE IF NOT EXISTS {} ({})").format(qualified_name, column_defs_sql)
        with self.conn.cursor() as cur:
            cur.execute(create_table_sql)
            self.conn.commit()

    def postgres_type(self, value):
        if isinstance(value, bool):
            return "BOOLEAN"
        if isinstance(value, int):
            return "INTEGER"
        if isinstance(value, float):
            return "DOUBLE PRECISION"
        return "TEXT"

    def insert_rows(self, table_name, rows):
        if not rows:
            return
        existing_columns = self.get_existing_columns(table_name)
        all_columns = set()
        for r in rows:
            all_columns.update(r.keys())
        column_order = sorted(list(set(existing_columns) & set(all_columns)))
        values = [[row.get(col, None) for col in column_order] for row in rows]
        qualified_name = sql.Identifier(self.schema, table_name)
        cols = sql.SQL(",").join(map(sql.Identifier, column_order))
        update_assignments = sql.SQL(", ").join(sql.SQL('{}=EXCLUDED.{}').format(sql.Identifier(col), sql.Identifier(col)) for col in column_order if col != "id")
        insert_sql = sql.SQL("INSERT INTO {} ({}) VALUES %s ON CONFLICT (id) DO UPDATE SET {}").format(qualified_name, cols, update_assignments)
        with self.conn.cursor() as cur:
            execute_values(cur, insert_sql, values, page_size=100)
            self.conn.commit()

    def get_existing_columns(self, table_name):
        sql_query = """
            SELECT column_name
            FROM information_schema.columns
            WHERE table_schema = %s
            AND table_name = %s;
        """
        with self.conn.cursor() as cur:
            cur.execute(sql_query, (self.schema, table_name))
            return [row[0] for row in cur.fetchall()]

    def delete_existing_data(self, table_name):
        sql_query = sql.SQL("DELETE FROM {}.{}").format(sql.Identifier(self.schema), sql.Identifier(table_name))
        with self.conn.cursor() as cur:
            cur.execute(sql_query)
            self.conn.commit()

    def write_metadata(self, table_name, rows):
        self.ensure_table(table_name, rows[0])
        sql_query = sql.SQL("INSERT INTO {}.{} ({}) VALUES %s").format(sql.Identifier(self.schema), sql.Identifier(table_name), sql.SQL(",").join(map(sql.Identifier, rows[0].keys())))
        values = [[row[col] for col in rows[0].keys()] for row in rows]
        with self.conn.cursor() as cur:
            execute_values(cur, sql_query, values, page_size=100)
            self.conn.commit()

    def load_metadata(self, table_name):
        sql_query = sql.SQL("SELECT * FROM {}.{}").format(sql.Identifier(self.schema), sql.Identifier(table_name))
        with self.conn.cursor() as cur:
            cur.execute(sql_query)
            rows = cur.fetchall()
            columns = [desc[0] for desc in cur.description]
            return [dict(zip(columns, row)) for row in rows]

    def update_metadata(self, table_name, rows):
        self.ensure_table(table_name, rows[0])
        sql_query = sql.SQL("INSERT INTO {}.{} ({}) VALUES %s ON CONFLICT (id) DO UPDATE SET {}").format(
            sql.Identifier(self.schema),
            sql.Identifier(table_name),
            sql.SQL(",").join(map(sql.Identifier, rows[0].keys())),
            sql.SQL(", ").join(sql.SQL('{}=EXCLUDED.{}').format(sql.Identifier(col), sql.Identifier(col)) for col in rows[0].keys() if col != "id")
        )
        values = [[row[col] for col in rows[0].keys()] for row in rows]
        with self.conn.cursor() as cur:
            execute_values(cur, sql_query, values, page_size=100)
            self.conn.commit()

    def close(self):
        self.conn.close()


sample_configfile.txt

xml_dir=C:\XML_Files
output_dir=C:\Output

# Configuration with custom metadata columns
INFORA02=[Details1_Collection], [metier], [CIB ITO IT Client Engagement & Protection IT], [id, ("SERVER_ID", "PAM_LOGINNAME")]
SA09=[Details2_Collection], [Metier2], [CIB ITO IT Client Engagement & Protection IT, CIB ITO IT CIB ITO CCCO], [id, ("SafeName")]

# Traditional configurations (will use default SafeName)
BG01=[Details], [Platform], [Windows Server Local], [id, ("SafeName")]
BG02=[Details], [Platform], [Windows Server Local]
INFUNIX03=[Details], [Platform], [Unix Local]



main.py

import argparse
from config_loader import load_config
from table_processor import TableProcessor
from db_manager import DBManager

def main():
    parser = argparse.ArgumentParser(description='Process XML files.')
    parser.add_argument('--table', choices=['main', 'metadata'], required=True)
    args = parser.parse_args()

    config, collection_configs = load_config('config.txt')

    xml_dir = config.get('xml_dir')
    output_dir = config.get('output_dir')

    if not xml_dir or not output_dir:
        print("Missing xml_dir or output_dir in config.txt")
        return

    db_manager = DBManager()
    table_processor = TableProcessor(xml_dir, output_dir, collection_configs, db_manager)

    if args.table == 'main':
        table_processor.process_main_table()
    elif args.table == 'metadata':
        table_processor.process_metadata_table()

    db_manager.close()

if __name__ == "__main__":
    main()

metadata_writer.py

from db_manager import DBManager

class MetadataWriter:
    def __init__(self, metadata_data, db_manager, table_name):
        self.metadata_data = metadata_data
        self.db_manager = db_manager
        self.table_name = table_name

    def write_metadata(self):
        self.db_manager.write_metadata(self.table_name, self.metadata_data)




















Custom Metadata Columns Implementation
Configuration Format
The configuration file now supports custom metadata column specifications in the format:
CONFIG_NAME=[collection], [attribute_name], [attribute_values], [id, ("COLUMN1", "COLUMN2", ...)]
Examples
Example 1: Multiple Columns (SERVER_ID and PAM_LOGINNAME)
INFORA02=[Details1_Collection], [metier], [CIB ITO IT Client Engagement & Protection IT], [id, ("SERVER_ID", "PAM_LOGINNAME")]
Metadata table for INFORA02 will have columns:

id (auto-generated unique ID)
SERVER_ID (extracted from XML attribute)
PAM_LOGINNAME (extracted from XML attribute)
SafeId (calculated based on SERVER_ID + PAM_LOGINNAME combination)
present_on (date when record was processed)

Example 2: Single Column (SafeName)
SA09=[Details2_Collection], [Metier2], [CIB ITO IT Client Engagement & Protection IT, CIB ITO IT CIB ITO CCCO], [id, ("SafeName")]
Metadata table for SA09 will have columns:

id (auto-generated unique ID)
SafeName (extracted from XML attribute)
SafeId (calculated based on SafeName)
present_on (date when record was processed)

Example 3: Default Behavior (no metadata specification)
BG02=[Details], [Platform], [Windows Server Local]
Metadata table for BG02 will have columns:

id (auto-generated unique ID)
SafeName (default - extracted from XML attribute)
SafeId (calculated based on SafeName)
present_on (date when record was processed)

Sample Data Output
INFORA02 metadata table:
idSERVER_IDPAM_LOGINNAMESafeIdpresent_on1SRV001admin0112024-09-042SRV002service0122024-09-043SRV001admin0112024-09-054SRV003backup0132024-09-05
SA09 metadata table:
idSafeNameSafeIdpresent_on1WindowsSafe112024-09-042UnixSafe122024-09-043WindowsSafe112024-09-054DatabaseSafe32024-09-05
Key Features
1. SafeId Consistency

The SafeId remains consistent for the same combination of metadata columns across different dates
For INFORA02: SERVER_ID="SRV001" + PAM_LOGINNAME="admin01" always gets SafeId=1
For SA09: SafeName="WindowsSafe1" always gets SafeId=1

2. Historical Tracking

Each day's run creates new entries with updated present_on dates
Previous data is never deleted or modified
You can track when specific combinations were present

3. Flexible Column Configuration

Support for single or multiple metadata columns
Case-insensitive attribute matching from XML
Automatic fallback to default "SafeName" if no specification provided

4. Duplicate Prevention

No duplicate combinations for the same date
Unique ID generation prevents database conflicts
Smart deduplication during parsing and insertion

Advanced SafeId Calculation
The SafeId is calculated based on the unique combination of the specified metadata columns:
python# For INFORA02 with ("SERVER_ID", "PAM_LOGINNAME"):
safe_key = (server_id.lower().strip(), pam_loginname.lower().strip())

# For SA09 with ("SafeName"):  
safe_key = (safename.lower().strip(),)
This ensures that:

Same combinations always get the same SafeId
Different combinations get unique SafeIds
The system handles case variations and whitespace

Database Schema Evolution
The metadata tables automatically adapt to the configured columns:
Traditional table (BG02):
sqlCREATE TABLE pam_cyberark_metadata_bg02 (
    id INTEGER PRIMARY KEY,
    SafeName TEXT,
    SafeId INTEGER,
    present_on TEXT
);
Custom multi-column table (INFORA02):
sqlCREATE TABLE pam_cyberark_metadata_infora02 (
    id INTEGER PRIMARY KEY,
    SERVER_ID TEXT,
    PAM_LOGINNAME TEXT,
    SafeId INTEGER,
    present_on TEXT
);
Usage Examples
Query all metadata for a specific server:
sqlSELECT * FROM pam_cyberark_metadata_infora02 
WHERE SERVER_ID = 'SRV001' 
ORDER BY present_on;
Find new safes that appeared on a specific date:
sqlSELECT DISTINCT SafeName FROM pam_cyberark_metadata_sa09 
WHERE present_on = '2024-09-05' 
AND SafeId NOT IN (
    SELECT DISTINCT SafeId FROM pam_cyberark_metadata_sa09 
    WHERE present_on < '2024-09-05'
);
Track login name changes for a server:
sqlSELECT SERVER_ID, PAM_LOGINNAME, present_on 
FROM pam_cyberark_metadata_infora02 
WHERE SERVER_ID = 'SRV001' 
ORDER BY present_on;





Summary
The updated code now supports fully customizable metadata columns through configuration. Here's what the implementation provides:

Flexible Column Configuration: Define any combination of XML attributes as metadata columns
Consistent SafeId Mapping: Same combinations always get the same SafeId across dates
Historical Data Preservation: All previous data remains intact while new entries are appended
Automatic Schema Creation: Database tables adapt to the configured columns
Robust Duplicate Prevention: No conflicts or duplicate entries for the same date

The key changes ensure that your metadata tables can track any combination of attributes (like SERVER_ID + PAM_LOGINNAME) while maintaining referential integrity through consistent SafeId assignments and proper historical tracking.
