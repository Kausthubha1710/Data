import os
import xml.etree.ElementTree as ET
import json
import argparse
from collections import OrderedDict
from datetime import date

def load_config(config_file):
    config = {}
    collection_configs = {}
    with open(config_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            if '=' in line:
                key, value = line.split('=', 1)
                config[key.strip()] = value.strip()
            elif ':' in line:
                key, value = line.split(':', 1)
                value = value.strip()
                try:
                    parts = [v.strip().strip('[]') for v in value.split('],')]
                    if len(parts) < 3:
                        print(f"Warning: Insufficient parameters for {key}")
                        continue
                    collection_name = parts[0]
                    attribute_name = parts[1]
                    attribute_values = [v.strip() for v in parts[2].split(',')]
                    collection_configs[key.strip()] = {
                        "collections": [collection_name],
                        "attribute_names": [attribute_name],
                        "attribute_values": attribute_values
                    }
                except Exception as e:
                    print(f"Error parsing line: {line} â€“ {e}")
    return config, collection_configs

def parse_xml_file(xml_file, collection_config):
    try:
        tree = ET.parse(xml_file)
        root = tree.getroot()
        namespace = root.tag.split('}')[0].strip('{')
        ns_prefix = f'{{{namespace}}}' if namespace else ''

        collection_element = root.find(f'.//{ns_prefix}{collection_config["collections"][0]}')
        if collection_element is None:
            print(f"Collection not found: {collection_config['collections'][0]} in {xml_file}")
            return None

        data = []
        elements = collection_element.findall(f'{ns_prefix}*')
        attribute_name = collection_config["attribute_names"][0]
        attribute_values = collection_config["attribute_values"]

        for element in elements:
            if attribute_name in element.attrib and element.attrib.get(attribute_name) in attribute_values:
                row = OrderedDict()
                row['id'] = len(data) + 1
                for attr in element.attrib:
                    row[attr] = element.attrib.get(attr, '')
                row['ImportDate'] = date.today().strftime("%Y-%m-%d")
                data.append(row)
        return data

    except Exception as e:
        print(f"Error parsing {xml_file}: {e}")
        return None

def parse_xml_file_metadata(xml_file, collection_config, output_dir, metadata_table_name, table_name):
    try:
        tree = ET.parse(xml_file)
        root = tree.getroot()
        namespace = root.tag.split('}')[0].strip('{')
        ns_prefix = f'{{{namespace}}}' if namespace else ''

        collection_element = root.find(f'.//{ns_prefix}{collection_config["collections"][0]}')
        if collection_element is None:
            print(f"Collection not found: {collection_config['collections'][0]} in {xml_file}")
            return None

        elements = collection_element.findall(f'{ns_prefix}*')
        attribute_name = collection_config["attribute_names"][0]
        attribute_values = collection_config["attribute_values"]

        json_file_path = os.path.join(output_dir, f'{table_name}.json')
        metadata_json_file_path = os.path.join(output_dir, f'{metadata_table_name}.json')

        try:
            with open(json_file_path, 'r', encoding='utf-8') as f:
                main_data = json.load(f)
        except FileNotFoundError:
            main_data = []

        try:
            with open(metadata_json_file_path, 'r', encoding='utf-8') as f:
                existing_metadata = json.load(f)
            last_id = max(item['id'] for item in existing_metadata) if existing_metadata else 0
        except FileNotFoundError:
            existing_metadata = []
            last_id = 0

        safe_id_map = {}
        next_safe_id = 1
        for row in main_data:
            key = (row['id'], row.get('SafeName', '').lower())
            if key not in safe_id_map:
                safe_id_map[key] = next_safe_id
                next_safe_id += 1

        metadata_data = []
        for element in elements:
            if attribute_name in element.attrib and element.attrib.get(attribute_name) in attribute_values:
                metadata_row = OrderedDict()
                safe_name = next((element.attrib.get(k, '') for k in element.attrib if k.lower() == 'safename'), '')
                metadata_row['id'] = last_id + 1
                key = (metadata_row['id'], safe_name.lower())
                if key not in safe_id_map:
                    safe_id_map[key] = next_safe_id
                    next_safe_id += 1
                metadata_row['SafeId'] = safe_id_map[key]
                metadata_row['SafeName'] = safe_name
                metadata_row['present_on'] = date.today().strftime("%Y-%m-%d")
                metadata_data.append(metadata_row)
                last_id += 1
        return metadata_data

    except Exception as e:
        print(f"Error parsing metadata from {xml_file}: {e}")
        return None

def write_data_to_json(data, output_filename):
    if data:
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=4)
        print(f"Data written to {output_filename}")
    else:
        print("No data to write.")

def load_metadata_json(json_file_path):
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return []

def update_metadata_json(metadata_data, json_file_path):
    existing_metadata = load_metadata_json(json_file_path)
    combined_metadata = existing_metadata + metadata_data
    with open(json_file_path, 'w', encoding='utf-8') as f:
        json.dump(combined_metadata, f, indent=4)
    print(f"Metadata updated in {json_file_path}")

def process_main_table(xml_dir, output_dir, collection_configs):
    for root, _, files in os.walk(xml_dir):
        for filename in files:
            if filename.endswith('.xml'):
                suffix = os.path.splitext(filename)[0].split('-')[-1].strip()
                table_name = f'pam_cyberarck_{suffix}'
                xml_file_path = os.path.join(root, filename)
                json_file_path = os.path.join(output_dir, f'{table_name}.json')
                collection_config = collection_configs.get(suffix)
                if not collection_config:
                    print(f"No config for file {filename}")
                    continue
                data = parse_xml_file(xml_file_path, collection_config)
                if data:
                    write_data_to_json(data, json_file_path)

def process_metadata_table(xml_dir, output_dir, collection_configs):
    for root, _, files in os.walk(xml_dir):
        for filename in files:
            if filename.endswith('.xml'):
                suffix = os.path.splitext(filename)[0].split('-')[-1].strip()
                table_name = f'pam_cyberarck_{suffix}'
                metadata_table_name = f'pam_cyberarck_metadata_{suffix}'
                xml_file_path = os.path.join(root, filename)
                metadata_json_file_path = os.path.join(output_dir, f'{metadata_table_name}.json')
                collection_config = collection_configs.get(suffix)
                if not collection_config:
                    print(f"No config for file {filename}")
                    continue
                metadata_data = parse_xml_file_metadata(xml_file_path, collection_config, output_dir, metadata_table_name, table_name)
                if metadata_data:
                    update_metadata_json(metadata_data, metadata_json_file_path)

def main():
    parser = argparse.ArgumentParser(description='Process XML files.')
    parser.add_argument('--table', choices=['main', 'metadata'], required=True)
    args = parser.parse_args()

    config, collection_configs = load_config('config.txt')
    xml_dir = config.get('xml_dir')
    output_dir = config.get('output_dir')

    if not xml_dir or not output_dir:
        print("Missing xml_dir or output_dir in config.txt")
        return

    os.makedirs(output_dir, exist_ok=True)

    if args.table == 'main':
        process_main_table(xml_dir, output_dir, collection_configs)
    elif args.table == 'metadata':
        process_metadata_table(xml_dir, output_dir, collection_configs)

if __name__ == "__main__":
    main()
