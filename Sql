Insert/Update Procedure Summary
The data management system involves two tables:

PAM_Cyberarck_<reportname> – stores the latest state of each record

PAM_Cyberarck_<reportname>_metadata – maintains a historical log of file arrivals

1. Insert into Metadata Table
Each time a new data file is processed, every record from that file is logged into the metadata table regardless of whether it already exists in the main table.
This ensures a complete history of all data arrivals.

Purpose: To track the presence of each SafeName on a particular date.

Operation: A new row is inserted into the metadata table with:

SafeId (linked to the main table)

SafeName

Current date as present_on

2. Insert or Update into Main Table
The main table stores the most recent version of each record (SafeName), ensuring up-to-date data.

If the record (SafeName) does not exist:

A new row is inserted with all relevant data fields.

The ImportDate is set to the current date.

If the record (SafeName) already exists:

The existing row is updated with new values from the incoming data.

The ImportDate is updated to reflect the most recent arrival date.















import os
import xml.etree.ElementTree as ET
import json
import argparse
from collections import OrderedDict
from datetime import date

def load_config(config_file):
    config = {}
    collection_configs = {}
    with open(config_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line and '=' in line:
                key, value = line.split('=', 1)
                config[key.strip()] = value.strip()
            elif ':' in line:
                filename, params = line.split(':', 1)
                collection, attribute_name, attribute_value = params.split(',')
                collection_configs[filename.strip()] = {
                    "collection": collection.strip(),
                    "attribute_name": attribute_name.strip(),
                    "attribute_value": attribute_value.strip()
                }
    return config, collection_configs

def parse_xml_file(xml_file, collection_config):
    tree = ET.parse(xml_file)
    root = tree.getroot()
    namespace = '{' + root.tag.split('}')[0].strip('{') + '}'

    collection_element = root.find(f'.//{namespace}{collection_config["collection"]}')
    if collection_element is None:
        print(f"Collection not found for tag {collection_config['collection']} in {xml_file}")
        return []

    data = []
    elements = collection_element.findall(f'{namespace}*')
    if not elements:
        print(f"No elements found for tag {collection_config['collection']} in {xml_file}")
        return []

    for element in elements:
        if (collection_config["attribute_name"] in element.attrib and
            element.attrib.get(collection_config["attribute_name"]) == collection_config["attribute_value"]):

            row = OrderedDict()
            row['Instance'] = element.attrib.get('Instance', '')
            row['Metier2'] = element.attrib.get('Metier2', '')
            row['SafeName'] = element.attrib.get('SafeName', '')
            row['BAM_ID'] = element.attrib.get('BAM_ID', '')
            row['AppName'] = element.attrib.get('AppName', '')
            row['AppStatus'] = element.attrib.get('AppStatus', '')
            row['EntitlementOwnerUID'] = element.attrib.get('EntitlementOwnerUID', '')
            row['EntitlementOwnerName'] = element.attrib.get('EntitlementOwnerName', '')
            row['EntitlementOwnerMetier'] = element.attrib.get('EntitlementOwnerMetier', '')
            row['EntitlementOwnerSite'] = element.attrib.get('EntitlementOwnerSite', '')
            row['NbSafeUsers'] = element.attrib.get('NbSafeUsers', '')
            row['NbSafeUsersWithOwnerRight'] = element.attrib.get('NbSafeUsersWithOwnerRight', '')
            row['NbSafeUsersWithBreakglassRight'] = element.attrib.get('NbSafeUsersWithBreakglassRight', '')
            row['NbSafeUsersWithDEVRight'] = element.attrib.get('NbSafeUsersWithDEVRight', '')
            row['NbSafeUserswithConsultRight'] = element.attrib.get('NbSafeUsersWithConsultRight', '')
            row['SafeCreationDate'] = element.attrib.get('SafeCreationDate', '')
            row['Assignee'] = element.attrib.get('Assignee', '')
            row['Action'] = element.attrib.get('Action', '')
            row['Remediation'] = element.attrib.get('Remediation', '')
            row['Comments'] = element.attrib.get('Comments', '')
            row['ImportDate'] = date.today().strftime("%Y-%m-%d")

            data.append(row)
    return data

def merge_with_existing_data(new_data, json_file_path):
    if os.path.exists(json_file_path):
        with open(json_file_path, 'r', encoding='utf-8') as f:
            existing_data = json.load(f)
    else:
        existing_data = []

    existing_data_by_key = { (row['SafeName']): row for row in existing_data }

    for row in new_data:
        existing_data_by_key[row['SafeName']] = row

    # Retain all records, only update ImportDate for matched records
    for key in existing_data_by_key:
        if key not in [r['SafeName'] for r in new_data]:
            existing_data_by_key[key]['ImportDate'] = date.today().strftime("%Y-%m-%d")

    merged_data = list(existing_data_by_key.values())
    # Reassign IDs starting from 1
    for idx, row in enumerate(sorted(merged_data, key=lambda x: x['SafeName'])):
        row['id'] = idx + 1

    return merged_data

def write_data_to_json(data, output_filename):
    with open(output_filename, 'w', encoding='utf-8') as json_file:
        json.dump(data, json_file, indent=4)
    print(f"Data written to {output_filename}")

def process_main_table(xml_dir, output_dir, collection_configs):
    for root_dir, dirs, files in os.walk(xml_dir):
        for filename in files:
            if filename.endswith('.xml'):
                suffix = os.path.splitext(filename)[0].split('-')[-1].strip()
                table_name = f'PAM_Cyberarck_reportname_{suffix}'
                json_file_path = os.path.join(output_dir, f'{table_name}.json')
                xml_file_path = os.path.join(root_dir, filename)

                collection_config = collection_configs.get(suffix)
                if not collection_config:
                    print(f"No collection config found for file {filename}")
                    continue

                new_data = parse_xml_file(xml_file_path, collection_config)
                merged_data = merge_with_existing_data(new_data, json_file_path)
                write_data_to_json(merged_data, json_file_path)

def main():
    parser = argparse.ArgumentParser(description='Process XML files')
    parser.add_argument('--table', choices=['main'], required=True, help='Specify which table to process')
    args = parser.parse_args()

    config, collection_configs = load_config('config.txt')
    xml_dir = config.get('xml_dir')
    output_dir = config.get('output_dir')

    if not xml_dir or not output_dir:
        print("Error: Missing xml_dir or output_dir in config.txt")
        return

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    if args.table == 'main':
        process_main_table(xml_dir, output_dir, collection_configs)

if __name__ == "__main__":
    main()
